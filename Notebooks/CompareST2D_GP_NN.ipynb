{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy\n",
    "from torchinfo import summary\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing surface temperature map\n",
    "raw_ST_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_ST2D.csv', delimiter=',')\n",
    "\n",
    "#Path to load NN model\n",
    "NN_model_load_path = base_dir+'Model_Storage/NN_ST_server_stand_norm/'\n",
    "\n",
    "#Path to store comparison plots\n",
    "plot_save_path = base_dir+'Plots/ST_GP_vs_NN/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_ST_data[:, :4] #has shape 46 x 72 = 3,312\n",
    "raw_outputs = raw_ST_data[:, 5:]\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features\n",
    "\n",
    "## HYPER-PARAMETERS ##\n",
    "#Defining partition of data used for 1. training, 2. validation and 3. testing\n",
    "data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 1\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "rng = torch.Generator(device=device)\n",
    "rng.manual_seed(partition_seed)\n",
    "\n",
    "# Variable to show plots or not \n",
    "show_plot = False\n",
    "\n",
    "## GP\n",
    "#Number of nearest neighbors to choose\n",
    "N_neigbors = 10\n",
    "\n",
    "#Distance metric to use\n",
    "distance_metric = 'euclidean' #options: 'euclidean', 'mahalanobis', 'logged_euclidean', 'logged_mahalanobis'\n",
    "\n",
    "#Convert raw inputs for H2 and CO2 pressures to log10 scale so don't have to deal with it later\n",
    "if 'logged' in distance_metric:\n",
    "    raw_inputs[:, 0] = np.log10(raw_inputs[:, 0]) #H2\n",
    "    raw_inputs[:, 1] = np.log10(raw_inputs[:, 1]) #CO2\n",
    "\n",
    "## NN\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#Regularization coefficient\n",
    "regularization_coeff = 0.0\n",
    "\n",
    "#Weight decay \n",
    "weight_decay = 0.0\n",
    "\n",
    "#Batch size \n",
    "batch_size = 200\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e71db2",
   "metadata": {},
   "source": [
    "# Partition data and build Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning DataModule\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_inputs, train_outputs, valid_inputs, valid_outputs, \n",
    "                 test_inputs, test_outputs, batch_size, rng, reshape_for_cnn=False, \n",
    "                 img_channels=1, img_height=None, img_width=None):\n",
    "        super().__init__()\n",
    "\n",
    "        #Store original shapes for reshaping \n",
    "        self.batch_size = batch_size\n",
    "        self.rng = rng\n",
    "        self.reshape_for_cnn = reshape_for_cnn\n",
    "        self.img_channels = img_channels\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        \n",
    "        # Standardizing the output\n",
    "        ## Create scaler\n",
    "        out_scaler = StandardScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        out_scaler.fit(train_outputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_outputs = torch.tensor(out_scaler.transform(train_outputs.numpy()), dtype=torch.float32)\n",
    "        valid_outputs = torch.tensor(out_scaler.transform(valid_outputs.numpy()), dtype=torch.float32)\n",
    "        test_outputs = torch.tensor(out_scaler.transform(test_outputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.out_scaler = out_scaler\n",
    "        \n",
    "        # Normalizing the input\n",
    "        ## Create scaler\n",
    "        in_scaler = MinMaxScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        in_scaler.fit(train_inputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_inputs = torch.tensor(in_scaler.transform(train_inputs.numpy()), dtype=torch.float32)\n",
    "        valid_inputs = torch.tensor(in_scaler.transform(valid_inputs.numpy()), dtype=torch.float32)\n",
    "        test_inputs = torch.tensor(in_scaler.transform(test_inputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.in_scaler = in_scaler\n",
    "        \n",
    "        #Store the inputs\n",
    "        self.train_inputs = train_inputs\n",
    "        self.valid_inputs = valid_inputs\n",
    "        self.test_inputs = test_inputs\n",
    "\n",
    "        # Reshape data if needed for CNN\n",
    "        if reshape_for_cnn:\n",
    "            # Reshape inputs\n",
    "            if img_height is None or img_width is None:\n",
    "                # Auto-calculate square dimensions if not provided\n",
    "                total_labels = train_outputs.shape[1]\n",
    "                img_size = int(np.sqrt(total_labels / img_channels))\n",
    "                if img_size * img_size * img_channels != total_labels:\n",
    "                    raise ValueError(f\"Cannot reshape {total_labels} features into square image. \"\n",
    "                                     f\"Please provide img_height and img_width explicitly.\")\n",
    "                self.img_height = img_size\n",
    "                self.img_width = img_size\n",
    "            \n",
    "            self.train_outputs = train_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_outputs = valid_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_outputs = test_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "\n",
    "        else:\n",
    "            self.train_outputs = train_outputs\n",
    "            self.valid_outputs = valid_outputs\n",
    "            self.test_outputs = test_outputs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_inputs, self.train_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, generator=self.rng)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.valid_inputs, self.valid_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_inputs, self.test_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "\n",
    "#Splitting the data \n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(N), data_partitions, generator=rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = torch.tensor(raw_inputs[train_idx], dtype=torch.float32)\n",
    "train_outputs = torch.tensor(raw_outputs[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "valid_inputs = torch.tensor(raw_inputs[valid_idx], dtype=torch.float32)\n",
    "valid_outputs = torch.tensor(raw_outputs[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "test_inputs = torch.tensor(raw_inputs[test_idx], dtype=torch.float32)\n",
    "test_outputs = torch.tensor(raw_outputs[test_idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataModule\n",
    "data_module = CustomDataModule(\n",
    "    train_inputs, train_outputs,\n",
    "    valid_inputs, valid_outputs,\n",
    "    test_inputs, test_outputs,\n",
    "    batch_size, rng, reshape_for_cnn=True,\n",
    "    img_channels=1, img_height=46, img_width=72\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d894c0",
   "metadata": {},
   "source": [
    "# Fit data with Ensemble Conditional GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f11a3",
   "metadata": {},
   "source": [
    "## Build Conditional GP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ccf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sai_CGP(obs_features, obs_labels, query_features):\n",
    "    \"\"\"\n",
    "    Conditional Gaussian Process\n",
    "    Inputs: \n",
    "        obs_features : ndarray (D, N)\n",
    "            D-dimensional features of the N ensemble data points.\n",
    "        obs_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the N ensemble data points.\n",
    "        query_features : ndarray (D, 1)\n",
    "            D-dimensional features of the query data point.\n",
    "    Outputs:\n",
    "        query_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the ensemble updated from the query point.\n",
    "        query_cov_labels : ndarray (K, K)\n",
    "            K-by-K covariance of the ensemble labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining relevant covariance matrices\n",
    "    ## Between feature and label of observation data\n",
    "    Cyx = (obs_labels @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and feature of observation data\n",
    "    Cxy = (obs_features @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between feature and feature of observation data\n",
    "    Cxx = (obs_features @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and label of observation data\n",
    "    Cyy = (obs_labels @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Adding regularizer to avoid singularities\n",
    "    Cxx += 1e-7 * np.eye(Cxx.shape[0]) \n",
    "\n",
    "    query_labels = obs_labels + (Cyx @ scipy.linalg.pinv(Cxx) @ (query_features - obs_features))\n",
    "\n",
    "    query_cov_labels = Cyy - Cyx @ scipy.linalg.pinv(Cxx) @ Cxy\n",
    "\n",
    "    return query_labels, query_cov_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be00d5f",
   "metadata": {},
   "source": [
    "## Evaluating GP approach on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store model outputs\n",
    "test_GP_outputs = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "GP_train_inputs = np.copy(train_inputs.numpy())\n",
    "GP_train_outputs = np.copy(train_outputs.numpy())\n",
    "\n",
    "for query_idx, (test_input, test_output) in enumerate(zip(test_inputs.numpy(), test_outputs.numpy())):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    # Euclidian distance\n",
    "    if 'euclidean' in distance_metric:\n",
    "        distances = np.sqrt( (test_input[0] - GP_train_inputs[:,0])**2 + (test_input[1] - GP_train_inputs[:,1])**2 + (test_input[2] - GP_train_inputs[:,2])**2 + (test_input[3] - GP_train_inputs[:,3])**2 )\n",
    "    # Mahalanobis distance\n",
    "    elif 'mahalanobis' in distance_metric:\n",
    "        distances = np.sqrt( (test_input - np.mean(GP_train_inputs, axis=0)).T @ scipy.linalg.inv((GP_train_inputs.T @ GP_train_inputs) / (GP_train_inputs.shape[0] - 1)) @ (test_input - np.mean(GP_train_inputs, axis=0)) )\n",
    "    else:raise('Invalid distance metric')\n",
    "    \n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = GP_train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs = GP_train_outputs[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, prox_train_outputs.T, test_input.reshape((1, 4)).T)\n",
    "    test_GP_outputs[query_idx, :] = np.mean(mean_test_output, axis=1)\n",
    "    model_test_output_err = np.sqrt(np.diag(cov_test_output))\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cov_test_output, cmap='coolwarm', origin='lower')\n",
    "        plt.colorbar(label='Covariance')\n",
    "        plt.title('Joint Covariance Matrix')\n",
    "        plt.xlabel('Output index')\n",
    "        plt.ylabel('Output index')\n",
    "        plt.show()\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_test_GP_output = test_GP_outputs[query_idx, :].reshape((46, 72))\n",
    "        plot_res = plot_test_GP_output - plot_test_output\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('Model')\n",
    "        hm2 = sns.heatmap(plot_test_GP_output, ax=ax2)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax3.set_title('Residuals')\n",
    "        hm3 = sns.heatmap(plot_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        # Shared colorbar (use the last heatmap's mappable)\n",
    "        # cbar = fig.colorbar(hm3.get_children()[0], ax=[ax1, ax2, ax3], location='right')\n",
    "        # cbar.set_label(\"Temperature\")\n",
    "        # Fix longitude ticks\n",
    "        ax3.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax3.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax3.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$ : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d07f8b",
   "metadata": {},
   "source": [
    "# Import CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bc504",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0377013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "\n",
    "        # Project input parameters to a higher dimension\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 128 * 6 * 9)  # 6x9 feature maps with 128 channels\n",
    "        )\n",
    "\n",
    "        # Decoder layers - progressively upsample\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Input: 128 x 6 x 9\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 128 x 12 x 18\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 64 x 24 x 36\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 32 x 48 x 72\n",
    "            \n",
    "            # Fine-tune to exact dimensions (48x72 -> 46x72)\n",
    "            nn.Conv2d(32, 16, kernel_size=(3,3), stride=1, padding=(0,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 16 x 46 x 72\n",
    "            \n",
    "            nn.Conv2d(16, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "            # Output: output_channels x 46 x 72\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Generated images of shape (batch_size, output_channels, 46, 72)\n",
    "        \"\"\"\n",
    "        # Project to higher dimension and reshape\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, 6, 9)  # Reshape to (batch, channels, height, width)\n",
    "        \n",
    "        # Decode to image\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf22ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(D,1).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6df18",
   "metadata": {},
   "source": [
    "## Define optimization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class RegressionModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, learning_rate, weight_decay=0.0, reg_coeff=0.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer_class = optimizer\n",
    "    \n",
    "    def compute_gradient_penalty(self, X):\n",
    "        \"\"\"\n",
    "        Compute the gradient of model output with respect to input.\n",
    "        Returns the L2 norm of the gradients as a regularization term.\n",
    "        \"\"\"\n",
    "        if self.reg_coeff == 0:\n",
    "            return torch.tensor(0., device=self.device)\n",
    "        \n",
    "        # Clone and enable gradient computation for inputs\n",
    "        X_grad = X.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Temporarily enable gradients (needed for validation/test steps)\n",
    "        with torch.enable_grad():\n",
    "            \n",
    "            # Compute output (need to recompute to track gradients w.r.t. X)\n",
    "            output = self.model(X_grad)\n",
    "            \n",
    "            # Compute gradients of output with respect to input\n",
    "            grad_outputs = torch.ones_like(output)\n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=output,\n",
    "                inputs=X_grad,\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=True,  # Keep computation graph for backprop\n",
    "                retain_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Compute L2 norm of gradients (squared)\n",
    "            gradient_penalty = torch.mean(gradients ** 2)\n",
    "        \n",
    "        return self.reg_coeff * gradient_penalty\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Add gradient regularization\n",
    "        grad_penalty = self.compute_gradient_penalty(X)\n",
    "        loss += grad_penalty\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a34f3",
   "metadata": {},
   "source": [
    "## Retrieve optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "lightning_module = RegressionModule(\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_coeff=regularization_coeff,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = CSVLogger(NN_model_load_path+'logs', name='NeuralNetwork')\n",
    "\n",
    "# Create Trainer and train\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    logger=logger,\n",
    "    deterministic=True  # For reproducibility\n",
    ")\n",
    "\n",
    "# Load model\n",
    "lightning_module = RegressionModule.load_from_checkpoint(\n",
    "    NN_model_load_path + f'{n_epochs}epochs_{regularization_coeff}WD_{regularization_coeff}RC_{learning_rate}LR_{batch_size}BS.ckpt',\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "learning_rate=learning_rate,\n",
    "reg_coeff=regularization_coeff,\n",
    "weight_decay=weight_decay\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b449f",
   "metadata": {},
   "source": [
    "## Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9396807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing predicted T-P profiles vs true T-P profiles with residuals\n",
    "substep = 100\n",
    "\n",
    "# Get the scalers from data module\n",
    "out_scaler = data_module.out_scaler\n",
    "in_scaler = data_module.in_scaler\n",
    "\n",
    "#Initialize array to store model outputs\n",
    "test_NN_outputs = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "for test_idx, (test_input, test_output) in enumerate(zip(test_inputs.numpy(), test_outputs.numpy())):\n",
    "\n",
    "    #Retrieve prediction\n",
    "    pred_output_scaled = model(torch.tensor(in_scaler.transform(test_input.reshape(1, -1)))).detach().numpy().reshape(3312)\n",
    "    \n",
    "    # Inverse transform to get original scale\n",
    "    test_NN_outputs[test_idx, :] = out_scaler.inverse_transform(pred_output_scaled.reshape(1, -1)).flatten()\n",
    "\n",
    "    #Plotting\n",
    "    if (test_idx % substep == 0):\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_pred_output = test_NN_outputs[test_idx, :].reshape((46, 72))\n",
    "        plot_res = plot_pred_output - plot_test_output\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax2.set_title('NN Model')\n",
    "        hm2 = sns.heatmap(plot_pred_output, ax=ax2, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax3.set_title('NN Residuals')\n",
    "        hm3 = sns.heatmap(plot_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax3.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax3.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax3.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$O : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d333d",
   "metadata": {},
   "source": [
    "# Compare both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b90491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing GP predicted T-P profiles vs NN predicted T-P profiles vs true T-P profiles\n",
    "substep = 100\n",
    "\n",
    "#Initialize array to store model residuals\n",
    "res_NN_outputs = np.zeros(test_outputs.shape, dtype=float)\n",
    "res_GP_outputs = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "for test_idx, (test_input, test_output) in enumerate(zip(test_inputs.numpy(), test_outputs.numpy())):\n",
    "    \n",
    "    #Store residuals \n",
    "    res_NN_outputs[test_idx, :] = test_NN_outputs[test_idx, :] - test_output\n",
    "    res_GP_outputs[test_idx, :] = test_GP_outputs[test_idx, :] - test_output\n",
    "\n",
    "    #Plotting\n",
    "    if (test_idx % substep == 0):\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_pred_NN_output = test_NN_outputs[test_idx, :].reshape((46, 72))\n",
    "        plot_pred_GP_output = test_GP_outputs[test_idx, :].reshape((46, 72))\n",
    "        plot_NN_res = plot_pred_NN_output - plot_test_output\n",
    "        plot_GP_res = plot_pred_GP_output - plot_test_output\n",
    "\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax2.set_title('GP Model')\n",
    "        hm2 = sns.heatmap(plot_pred_NN_output, ax=ax2, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax3.set_title('GP Residuals')\n",
    "        hm3 = sns.heatmap(plot_NN_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax4.set_title('NN Model')\n",
    "        hm4 = sns.heatmap(plot_pred_GP_output, ax=ax4, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm4.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax5.set_title('NN Residuals')\n",
    "        hm5 = sns.heatmap(plot_GP_res, ax=ax5)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm5.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax5.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax5.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax5.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "\n",
    "        plt.suptitle(rf'H$_2$O : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{test_idx}.pdf')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ffd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- GP Residuals ---')\n",
    "print(f'Median = {np.median(res_GP_outputs):.4f} K, Std = {np.std(res_GP_outputs):.4f} K')\n",
    "print('\\n','--- NN Residuals ---')\n",
    "print(f'Median = {np.median(res_NN_outputs):.4f} K, Std = {np.std(res_NN_outputs):.4f} K')\n",
    "\n",
    "#Plot residuals\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=[12, 8], sharex=True, sharey=True)\n",
    "ax1.plot(np.mean(res_GP_outputs, axis=1), color='green')\n",
    "ax2.plot(np.mean(res_NN_outputs, axis=1), color='red')\n",
    "ax1.set_title('GP RESIDUALS')\n",
    "ax2.set_title('NN RESIDUALS')\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.axhline(0, color='black', linestyle='dashed')\n",
    "    ax.grid()\n",
    "ax1.set_xlabel('Index')\n",
    "ax2.set_xlabel('Index')\n",
    "ax1.set_ylabel('Temperature')\n",
    "ax2.set_ylabel('Temperature')\n",
    "\n",
    "# Add statistics text at the bottom\n",
    "stats_text = (\n",
    "    f\"--- GP Residuals ---\\n\"\n",
    "    f'Median = {np.median(res_GP_outputs):.4f} K, Std = {np.std(res_GP_outputs):.4f} K'\n",
    "    f\"\\n\"\n",
    "    f\"--- NN Residuals ---\\n\"\n",
    "    f'Median = {np.median(res_NN_outputs):.4f} K, Std = {np.std(res_NN_outputs):.4f} K'\n",
    ")\n",
    "\n",
    "fig.text(0.1, -0.02, stats_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig(plot_save_path+f'/res_GP_vs_NN.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d5b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
