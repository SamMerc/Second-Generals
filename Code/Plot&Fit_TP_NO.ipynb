{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import celerite2\n",
    "from celerite2 import terms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File containing temperature values\n",
    "raw_T_data = np.loadtxt('/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/Data/bt-4500k/training_data_T.csv', delimiter=',')\n",
    "#File containing pressure values\n",
    "raw_P_data = np.loadtxt('/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/Data/bt-4500k/training_data_P.csv', delimiter=',')\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure)\n",
    "raw_inputs = raw_T_data[:, :5]\n",
    "raw_outputs_T = raw_T_data[:, 5:]\n",
    "raw_outputs_P = raw_P_data[:, 5:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91916a9",
   "metadata": {},
   "source": [
    "# Plotting of the T-P profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_input, raw_output_T, raw_output_P in zip(raw_inputs,raw_outputs_T,raw_outputs_P):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n",
    "    ax.plot(raw_output_T, np.log(raw_output_P/1000), color='blue', linewidth=2)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Temperature (K)')\n",
    "    ax.set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "    ax.set_title(rf'H$_2$O : {raw_input[0]} bar, CO$_2$ : {raw_input[1]} bar, LoD : {raw_input[2]:.0f} days, Obliquity : {raw_input[3]} deg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3f340",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b3f486",
   "metadata": {},
   "source": [
    "# Fitting the training data with a transformer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090012",
   "metadata": {},
   "source": [
    "## First step : Define a training, validation, and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78466da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining partition of data used for 1. training 2. validation and 3. testing\n",
    "data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "\n",
    "#Splitting the data \n",
    "## Setting noise seec\n",
    "generator = torch.Generator().manual_seed(partition_seed)\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(raw_outputs_T.shape[0]), data_partitions, generator=generator)\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = raw_inputs[train_idx]\n",
    "train_outputs_T = raw_outputs_T[train_idx]\n",
    "train_outputs_P = raw_outputs_P[train_idx]\n",
    "### Validation\n",
    "valid_inputs = raw_inputs[valid_idx]\n",
    "valid_outputs_T = raw_outputs_T[valid_idx]\n",
    "valid_outputs_P = raw_outputs_P[valid_idx]\n",
    "### Testing\n",
    "test_inputs = raw_inputs[test_idx]\n",
    "test_outputs_T = raw_outputs_T[test_idx]\n",
    "test_outputs_P = raw_outputs_P[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d8d92",
   "metadata": {},
   "source": [
    "## Second step : defining the transformer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb57fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Defining transformer hyper-parameters\n",
    "# ---------------------\n",
    "## Number of attention heads\n",
    "att_heads = 16\n",
    "## Number of encoder/decoder layers \n",
    "num_layers = 12\n",
    "## Dimensionality of internal embeddings (i.e. transformer width)\n",
    "d_model = 64\n",
    "## Dimension of the feedforward network model (i.e. size of hidden layer)\n",
    "dim_feedforward = 128\n",
    "## Number of inputs\n",
    "src_len = 4\n",
    "## Number of outputs\n",
    "tgt_len = 51\n",
    "\n",
    "# ---------------------\n",
    "# Defining the model\n",
    "# ---------------------\n",
    "## Layers\n",
    "src_embed = torch.nn.Linear(1, d_model)   # for 4 inputs as a sequence\n",
    "tgt_embed = torch.nn.Linear(1, d_model)   # for 51 outputs as a sequence\n",
    "out_proj  = torch.nn.Linear(d_model, 1)   # back to scalar temps\n",
    "\n",
    "src_pos = torch.nn.Parameter(torch.randn(1, src_len, d_model))\n",
    "tgt_pos = torch.nn.Parameter(torch.randn(1, tgt_len, d_model))\n",
    "\n",
    "transformer = torch.nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=att_heads,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(list(src_embed.parameters()) +\n",
    "                       list(tgt_embed.parameters()) +\n",
    "                       list(out_proj.parameters()) +\n",
    "                       [src_pos, tgt_pos] +\n",
    "                       list(transformer.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55febc",
   "metadata": {},
   "source": [
    "## Third step: running the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c426b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# prepare src: (batch, src_len, 1) -> embed -> add pos\u001b[39;00m\n\u001b[32m      8\u001b[39m src = torch.as_tensor(test_inputs, dtype=torch.float64).unsqueeze(-\u001b[32m1\u001b[39m)                   \u001b[38;5;66;03m# (N,4,1)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m src = \u001b[43msrc_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m + src_pos          \u001b[38;5;66;03m# (N,4,d_model)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# prepare tgt input (here just zeros, could also use shifted Y)\u001b[39;00m\n\u001b[32m     12\u001b[39m Y_in = torch.as_tensor(test_outputs_T, dtype=torch.float64)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/MLenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/MLenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/MLenv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Training loop\n",
    "# ---------------------\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prepare src: (batch, src_len, 1) -> embed -> add pos\n",
    "    src = torch.as_tensor(test_inputs, dtype=torch.float64).unsqueeze(-1)                   # (N,4,1)\n",
    "    src = src_embed(src) + src_pos          # (N,4,d_model)\n",
    "\n",
    "    # prepare tgt input (here just zeros, could also use shifted Y)\n",
    "    Y_in = torch.as_tensor(test_outputs_T, dtype=torch.float64)\n",
    "    tgt = Y_in.unsqueeze(-1)                # (N,51,1)\n",
    "    tgt = tgt_embed(tgt) + tgt_pos          # (N,51,d_model)\n",
    "\n",
    "    # run through transformer\n",
    "    h = transformer(src, tgt)               # (N,51,d_model)\n",
    "\n",
    "    # project to temps\n",
    "    Y_pred = out_proj(h).squeeze(-1)        # (N,51)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(Y_pred, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706416c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
