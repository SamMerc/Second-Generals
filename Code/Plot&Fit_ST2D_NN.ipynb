{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing surface temperature map\n",
    "raw_ST_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_ST2D.csv', delimiter=',')\n",
    "#Path to store model\n",
    "model_save_path = base_dir+'Model_Storage/NN_ST_server_stand_norm/'\n",
    "check_and_make_dir(model_save_path)\n",
    "#Path to store plots\n",
    "plot_save_path = base_dir+'Plots/NN_ST_server_stand_norm/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_ST_data[:, :4] #has shape 46 x 72 = 3,312\n",
    "raw_outputs = raw_ST_data[:, 5:]\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66e740",
   "metadata": {},
   "source": [
    "# Defining hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining partition of data used for 1. training, 2. validation and 3. testing\n",
    "data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 1\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "partition_rng = torch.Generator(device=device)\n",
    "partition_rng.manual_seed(partition_seed)\n",
    "\n",
    "#Defining the noise seed for the generating of batches from the partitioned data\n",
    "batch_seed = 5\n",
    "batch_rng = torch.Generator(device=device)\n",
    "batch_rng.manual_seed(batch_seed)\n",
    "\n",
    "#Defining the noise seed for the neural network initialization\n",
    "NN_seed = 6\n",
    "NN_rng = torch.Generator(device=device)\n",
    "NN_rng.manual_seed(NN_seed)\n",
    "\n",
    "# Variable to show plots or not \n",
    "show_plot = False\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#Regularization coefficient\n",
    "regularization_coeff = 0.0\n",
    "\n",
    "#Weight decay \n",
    "weight_decay = 0.0\n",
    "\n",
    "#Batch size \n",
    "batch_size = 200\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 2000\n",
    "\n",
    "#Mode for optimization\n",
    "run_mode = 'reuse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3f486",
   "metadata": {},
   "source": [
    "# Fitting the training data with a basic deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090012",
   "metadata": {},
   "source": [
    "## First step : Define a training, validation, and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78466da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning DataModule\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_inputs, train_outputs, valid_inputs, valid_outputs, \n",
    "                 test_inputs, test_outputs, batch_size, rng, reshape_for_cnn=False, \n",
    "                 img_channels=1, img_height=None, img_width=None):\n",
    "        super().__init__()\n",
    "\n",
    "        #Store original shapes for reshaping \n",
    "        self.batch_size = batch_size\n",
    "        self.rng = rng\n",
    "        self.reshape_for_cnn = reshape_for_cnn\n",
    "        self.img_channels = img_channels\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        \n",
    "        # Standardizing the output\n",
    "        ## Create scaler\n",
    "        out_scaler = StandardScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        out_scaler.fit(train_outputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_outputs = torch.tensor(out_scaler.transform(train_outputs.numpy()), dtype=torch.float32)\n",
    "        valid_outputs = torch.tensor(out_scaler.transform(valid_outputs.numpy()), dtype=torch.float32)\n",
    "        test_outputs = torch.tensor(out_scaler.transform(test_outputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.out_scaler = out_scaler\n",
    "        \n",
    "        # Normalizing the input\n",
    "        ## Create scaler\n",
    "        in_scaler = MinMaxScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        in_scaler.fit(train_inputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_inputs = torch.tensor(in_scaler.transform(train_inputs.numpy()), dtype=torch.float32)\n",
    "        valid_inputs = torch.tensor(in_scaler.transform(valid_inputs.numpy()), dtype=torch.float32)\n",
    "        test_inputs = torch.tensor(in_scaler.transform(test_inputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.in_scaler = in_scaler\n",
    "        \n",
    "        #Store the inputs\n",
    "        self.train_inputs = train_inputs\n",
    "        self.valid_inputs = valid_inputs\n",
    "        self.test_inputs = test_inputs\n",
    "\n",
    "        # Reshape data if needed for CNN\n",
    "        if reshape_for_cnn:\n",
    "            # Reshape inputs\n",
    "            if img_height is None or img_width is None:\n",
    "                # Auto-calculate square dimensions if not provided\n",
    "                total_labels = train_outputs.shape[1]\n",
    "                img_size = int(np.sqrt(total_labels / img_channels))\n",
    "                if img_size * img_size * img_channels != total_labels:\n",
    "                    raise ValueError(f\"Cannot reshape {total_labels} features into square image. \"\n",
    "                                     f\"Please provide img_height and img_width explicitly.\")\n",
    "                self.img_height = img_size\n",
    "                self.img_width = img_size\n",
    "            \n",
    "            self.train_outputs = train_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_outputs = valid_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_outputs = test_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "\n",
    "        else:\n",
    "            self.train_outputs = train_outputs\n",
    "            self.valid_outputs = valid_outputs\n",
    "            self.test_outputs = test_outputs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_inputs, self.train_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, generator=self.rng)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.valid_inputs, self.valid_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_inputs, self.test_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "\n",
    "#Splitting the data \n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(N), data_partitions, generator=partition_rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = torch.tensor(raw_inputs[train_idx], dtype=torch.float32)\n",
    "train_outputs = torch.tensor(raw_outputs[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "valid_inputs = torch.tensor(raw_inputs[valid_idx], dtype=torch.float32)\n",
    "valid_outputs = torch.tensor(raw_outputs[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "test_inputs = torch.tensor(raw_inputs[test_idx], dtype=torch.float32)\n",
    "test_outputs = torch.tensor(raw_outputs[test_idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataModule\n",
    "data_module = CustomDataModule(\n",
    "    train_inputs, train_outputs,\n",
    "    valid_inputs, valid_outputs,\n",
    "    test_inputs, test_outputs,\n",
    "    batch_size, batch_rng, reshape_for_cnn=True,\n",
    "    img_channels=1, img_height=46, img_width=72\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa2735",
   "metadata": {},
   "source": [
    "## Second step : Define the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_channels, generator=None):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Set seed if generator provided\n",
    "        if generator is not None:\n",
    "            torch.manual_seed(generator.initial_seed())\n",
    "\n",
    "        # Project input parameters to a higher dimension\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 128 * 6 * 9)  # 6x9 feature maps with 128 channels\n",
    "        )\n",
    "\n",
    "        # Decoder layers - progressively upsample\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Input: 128 x 6 x 9\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 128 x 12 x 18\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 64 x 24 x 36\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 32 x 48 x 72\n",
    "            \n",
    "            # Fine-tune to exact dimensions (48x72 -> 46x72)\n",
    "            nn.Conv2d(32, 16, kernel_size=(3,3), stride=1, padding=(0,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 16 x 46 x 72\n",
    "            \n",
    "            nn.Conv2d(16, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "            # Output: output_channels x 46 x 72\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Generated images of shape (batch_size, output_channels, 46, 72)\n",
    "        \"\"\"\n",
    "        # Project to higher dimension and reshape\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, 6, 9)  # Reshape to (batch, channels, height, width)\n",
    "        \n",
    "        # Decode to image\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa87254",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(D,1, generator=NN_rng).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c607115",
   "metadata": {},
   "source": [
    "## Fourth step : Define optimization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class RegressionModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, learning_rate, weight_decay=0.0, reg_coeff=0.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer_class = optimizer\n",
    "    \n",
    "    def compute_gradient_penalty(self, X):\n",
    "        \"\"\"\n",
    "        Compute the gradient of model output with respect to input.\n",
    "        Returns the L2 norm of the gradients as a regularization term.\n",
    "        \"\"\"\n",
    "        if self.reg_coeff == 0:\n",
    "            return torch.tensor(0., device=self.device)\n",
    "        \n",
    "        # Clone and enable gradient computation for inputs\n",
    "        X_grad = X.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Temporarily enable gradients (needed for validation/test steps)\n",
    "        with torch.enable_grad():\n",
    "            \n",
    "            # Compute output (need to recompute to track gradients w.r.t. X)\n",
    "            output = self.model(X_grad)\n",
    "            \n",
    "            # Compute gradients of output with respect to input\n",
    "            grad_outputs = torch.ones_like(output)\n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=output,\n",
    "                inputs=X_grad,\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=True,  # Keep computation graph for backprop\n",
    "                retain_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Compute L2 norm of gradients (squared)\n",
    "            gradient_penalty = torch.mean(gradients ** 2)\n",
    "        \n",
    "        return self.reg_coeff * gradient_penalty\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Add gradient regularization\n",
    "        grad_penalty = self.compute_gradient_penalty(X)\n",
    "        loss += grad_penalty\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d7f7",
   "metadata": {},
   "source": [
    "## Fifth step : Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "lightning_module = RegressionModule(\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_coeff=regularization_coeff,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = CSVLogger(model_save_path+'logs', name='NeuralNetwork')\n",
    "\n",
    "# Set all seeds for complete reproducibility\n",
    "pl.seed_everything(NN_seed, workers=True)\n",
    "\n",
    "# Create Trainer and train\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    logger=logger,\n",
    "    deterministic=True  # For reproducibility\n",
    ")\n",
    "\n",
    "if run_mode == 'use':\n",
    "    \n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    # Save model (PyTorch Lightning style)\n",
    "    trainer.save_checkpoint(model_save_path + f'{n_epochs}epochs_{regularization_coeff}WD_{regularization_coeff}RC_{learning_rate}LR_{batch_size}BS.ckpt')\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "else:\n",
    "    # Load model\n",
    "    lightning_module = RegressionModule.load_from_checkpoint(\n",
    "        model_save_path + f'{n_epochs}epochs_{regularization_coeff}WD_{regularization_coeff}RC_{learning_rate}LR_{batch_size}BS.ckpt',\n",
    "        model=model,\n",
    "        optimizer=SGD,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_coeff=regularization_coeff,\n",
    "    weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model on test dataset\n",
    "if run_mode == 'use':trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8bfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accessing Training History After Training ---\n",
    "# Find the version directory (e.g., version_0, version_1, etc.)\n",
    "log_dir = model_save_path+'logs/NeuralNetwork'\n",
    "versions = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "latest_version = sorted(versions)[-1]  # Get the latest version\n",
    "csv_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "\n",
    "# Read the metrics\n",
    "metrics_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract losses per epoch\n",
    "train_losses = metrics_df[metrics_df['train_loss_epoch'].notna()]['train_loss_epoch'].tolist()\n",
    "eval_losses = metrics_df[metrics_df['valid_loss'].notna()]['valid_loss'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9276f73",
   "metadata": {},
   "source": [
    "## Sixth step : Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955237ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[3, 1]}, figsize=(10, 6))\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "n_batches = len(train_losses) // n_epochs\n",
    "\n",
    "# Create x-axis in terms of epochs (0 to n_epochs)\n",
    "x_all = np.linspace(0, n_epochs, len(train_losses))\n",
    "x_epoch = np.arange(n_epochs+1)\n",
    "\n",
    "# Plot transparent background showing all batch losses\n",
    "ax1.plot(x_all, train_losses, alpha=0.3, color='C0', linewidth=0.5)\n",
    "ax1.plot(x_all, eval_losses, alpha=0.3, color='C1', linewidth=0.5)\n",
    "\n",
    "# Plot solid lines showing epoch-level losses (every n_batches steps)\n",
    "train_epoch = [train_losses[0]] + train_losses[n_batches-1::n_batches]  # Last batch of each epoch\n",
    "eval_epoch = [eval_losses[0]] + eval_losses[n_batches-1::n_batches]\n",
    "ax1.plot(x_epoch, train_epoch, label=\"Train\", color='C0', linewidth=2, marker='o')\n",
    "ax1.plot(x_epoch, eval_epoch, label=\"Validation\", color='C1', linewidth=2, marker='o')\n",
    "\n",
    "# Same for difference plot\n",
    "diff_all = np.array(train_losses) - np.array(eval_losses)\n",
    "diff_epoch = np.array(train_epoch) - np.array(eval_epoch)\n",
    "\n",
    "ax2.plot(x_all, np.abs(diff_all), alpha=0.3, color='C2', linewidth=0.5)\n",
    "ax2.plot(x_epoch, np.abs(diff_epoch), color='C2', linewidth=2, marker='o')\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax2.set_ylabel(\"Loss Diff.\")\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.savefig(plot_save_path+'/loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing predicted T-P profiles vs true T-P profiles with residuals\n",
    "substep = 500\n",
    "\n",
    "# Get the scalers from data module\n",
    "out_scaler = data_module.out_scaler\n",
    "in_scaler = data_module.in_scaler\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(test_outputs) != np.ndarray):\n",
    "    test_outputs = test_outputs.numpy()\n",
    "\n",
    "res = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "for test_idx, (test_input, test_output) in enumerate(zip(test_inputs, test_outputs)):\n",
    "\n",
    "    #Convert to numpy\n",
    "    test_input = test_input.numpy()\n",
    "\n",
    "    #Retrieve prediction\n",
    "    pred_output_scaled = model(torch.tensor(in_scaler.transform(test_input.reshape(1, -1)))).detach().numpy().reshape(3312)\n",
    "    \n",
    "    # Inverse transform to get original scale\n",
    "    pred_output = out_scaler.inverse_transform(pred_output_scaled.reshape(1, -1)).flatten()\n",
    "\n",
    "    #Storing residuals \n",
    "    res[test_idx, :] = pred_output - test_output\n",
    "\n",
    "    #Plotting\n",
    "    if (test_idx % substep == 0):\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_pred_output = pred_output.reshape((46, 72))\n",
    "        plot_res = res[test_idx, :].reshape((46, 72))\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax2.set_title('NN Model')\n",
    "        hm2 = sns.heatmap(plot_pred_output, ax=ax2, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax3.set_title('NN Residuals')\n",
    "        hm3 = sns.heatmap(plot_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax3.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax3.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax3.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$O : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{test_idx}.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d90b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
