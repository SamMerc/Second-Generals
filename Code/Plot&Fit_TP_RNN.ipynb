{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing temperature values\n",
    "raw_T_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_T.csv', delimiter=',')\n",
    "#File containing pressure values\n",
    "raw_P_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_P.csv', delimiter=',')\n",
    "#Path to store model\n",
    "model_save_path = base_dir+'Model_Storage/RNN/'\n",
    "check_and_make_dir(model_save_path)\n",
    "#Path to store plots\n",
    "plot_save_path = base_dir+'Plots/RNN/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_T_data[:, :4]\n",
    "raw_outputs_T = raw_T_data[:, 5:]\n",
    "raw_outputs_P = raw_P_data[:, 5:]\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features\n",
    "O = raw_outputs_T.shape[1] #Number of outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb328de7",
   "metadata": {},
   "source": [
    "# Define hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df33ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device with 6 threads\n"
     ]
    }
   ],
   "source": [
    "#Defining partition of data used for 1. training 2. validation and 3. testing\n",
    "data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 6\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed\n",
    "partition_seed = 4\n",
    "rng = torch.Generator(device=device)\n",
    "rng.manual_seed(partition_seed)\n",
    "\n",
    "#Neural network width and depth\n",
    "nn_width = 100\n",
    "nn_depth = 5\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-5\n",
    "\n",
    "#Batch size \n",
    "batch_size = 64\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 1000\n",
    "\n",
    "#Define storage for losses\n",
    "train_losses = []\n",
    "eval_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3f486",
   "metadata": {},
   "source": [
    "# Fitting the training data with a recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090012",
   "metadata": {},
   "source": [
    "## First step : Define a training, validation, and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78466da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data \n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(N), data_partitions, generator=rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = torch.tensor(raw_inputs[train_idx], dtype=torch.float32)\n",
    "train_outputs_T = torch.tensor(raw_outputs_T[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "valid_inputs = torch.tensor(raw_inputs[valid_idx], dtype=torch.float32)\n",
    "valid_outputs_T = torch.tensor(raw_outputs_T[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "test_inputs = torch.tensor(raw_inputs[test_idx], dtype=torch.float32)\n",
    "test_outputs_T = torch.tensor(raw_outputs_T[test_idx], dtype=torch.float32)\n",
    "test_outputs_P = torch.tensor(raw_outputs_P[test_idx], dtype=torch.float32)\n",
    "\n",
    "##Generating data loaders\n",
    "train_dataloader = DataLoader(TensorDataset(train_inputs,train_outputs_T), batch_size=64, generator=rng, shuffle=True)\n",
    "eval_dataloader = DataLoader(TensorDataset(valid_inputs,valid_outputs_T), batch_size=64, generator=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa2735",
   "metadata": {},
   "source": [
    "## Second step : Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3491d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Variable-depth recurrent cell\n",
    "# ============================================================\n",
    "class RecurrentNeuralNetworkCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        # First layer: takes input + hidden state\n",
    "        self.input_layer = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "\n",
    "        # Create a list of additional hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(depth - 1)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = torch.cat((x, h_prev), dim=1)\n",
    "\n",
    "        # First layer\n",
    "        h = torch.tanh(self.input_layer(combined))\n",
    "\n",
    "        # Additional hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            h = torch.tanh(layer(h))\n",
    "\n",
    "        # Output\n",
    "        y = self.output_layer(h)\n",
    "        return y, h\n",
    "\n",
    "# ============================================================\n",
    "# Multi-step RNN wrapper\n",
    "# ============================================================\n",
    "class DeepRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.cell = RecurrentNeuralNetworkCell( input_dim, hidden_dim, output_dim, depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "            y_seq: (batch_size, seq_len, output_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]          # (batch_size, input_size)\n",
    "            y_t, h = self.cell(x_t, h)\n",
    "            outputs.append(y_t.unsqueeze(1))  # keep sequence dimension\n",
    "\n",
    "        y_seq = torch.cat(outputs, dim=1)     # (batch_size, seq_len, output_size)\n",
    "        return y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa87254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepRNN(\n",
      "  (cell): RecurrentNeuralNetworkCell(\n",
      "    (input_layer): Linear(in_features=104, out_features=100, bias=True)\n",
      "    (hidden_layers): ModuleList(\n",
      "      (0-3): 4 x Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (output_layer): Linear(in_features=100, out_features=51, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepRNN(D, nn_width, O, nn_depth).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c607115",
   "metadata": {},
   "source": [
    "## Fourth step : Define optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d568dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training loop ---\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss+=loss.item()\n",
    "        print(f\"Train loss: {loss.item():>7f}  [{batch * batch_size + len(X):>5d}/{size:>5d}]\")\n",
    "\n",
    "    #Store loss\n",
    "    train_losses.append(total_loss/len(dataloader))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Evaluation loop ---\n",
    "def eval_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    eval_loss = 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            eval_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    #Store loss\n",
    "    eval_loss /= num_batches\n",
    "    eval_losses.append(eval_loss)\n",
    "    print(f\"Eval loss={eval_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2d7f7",
   "metadata": {},
   "source": [
    "## Fifth step : Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1514d6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     eval_loop(eval_dataloader, model, loss_fn)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(dataloader, model, loss_fn, optimizer)\u001b[39m\n\u001b[32m      7\u001b[39m total_loss=\u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     loss = loss_fn(pred, y)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/MLenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/MLenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mDeepRNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     45\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m    x: (batch_size, seq_len, input_size)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m        y_seq: (batch_size, seq_len, output_size)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     batch_size, seq_len, _ = x.shape\n\u001b[32m     51\u001b[39m     device = x.device\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Initialize hidden state\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# --- Loss and optimizer ---\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Loss and optimizer ---\n",
    "for t in range(n_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    eval_loop(eval_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "#Save model \n",
    "torch.save(model.state_dict(), model_save_path + f'{n_epochs}epochs_{learning_rate}LR_{batch_size}BS.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9276f73",
   "metadata": {},
   "source": [
    "## Sixth step : Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955237ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[3, 1]}, figsize=(10, 6))\n",
    "ax1.plot(np.arange(n_epochs), train_losses, label=\"Train\")\n",
    "ax1.plot(np.arange(n_epochs), eval_losses, label=\"Validation\")\n",
    "ax2.plot(np.arange(n_epochs), np.array(train_losses) - np.array(eval_losses), label=\"Train\")\n",
    "ax1.set_yscale('log')\n",
    "# ax2.set_yscale('log')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax2.set_ylabel(\"Loss Diff.\")\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.savefig(plot_save_path+'/loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing predicted T-P profiles vs true T-P profiles with residuals\n",
    "substep = 1000\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(test_outputs_T) != np.ndarray):\n",
    "    test_outputs_T = test_outputs_T.detach().cpu().numpy()\n",
    "    test_outputs_P = test_outputs_P.detach().cpu().numpy()\n",
    "\n",
    "for test_idx, (test_input, test_output_T, test_output_P) in enumerate(zip(test_inputs, test_outputs_T, test_outputs_P)):\n",
    "\n",
    "    #Retrieve prediction\n",
    "    pred_output_T = model(test_input.reshape(1, 1, D)).detach().numpy()\n",
    "    pred_output_T = pred_output_T.reshape(O)\n",
    "\n",
    "    #Convert to numpy\n",
    "    test_input = test_input.numpy()\n",
    "\n",
    "    #Plotting\n",
    "    if (test_idx % substep == 0):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[8, 6], sharey=True, gridspec_kw = {'width_ratios':[3, 1]})\n",
    "        ax1.plot(test_output_T, np.log10(test_output_P/1000), '.', linestyle='-', color='blue', linewidth=2)\n",
    "        ax1.plot(pred_output_T, np.log10(test_output_P/1000), color='green', linewidth=2)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "        ax1.set_xlabel('Temperature (K)')\n",
    "        ax2.plot(pred_output_T - test_output_T, np.log10(test_output_P/1000), '.', linestyle='-', color='green', linewidth=2)\n",
    "        ax2.set_xlabel('Residuals (K)')\n",
    "        plt.suptitle(rf'H$_2$O : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{test_idx}.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all residuals \n",
    "\n",
    "#Storage\n",
    "residuals = np.zeros(test_outputs_T.shape,  dtype=object)\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(test_outputs_T) != np.ndarray):\n",
    "    test_outputs_T = test_outputs_T.numpy()\n",
    "\n",
    "for test_idx, (test_input, test_output_T) in enumerate(zip(test_inputs, test_outputs_T)):\n",
    "\n",
    "    #Retrieve prediction\n",
    "    residuals[test_idx] = model(test_input.reshape(1, 1, D)).detach().numpy().reshape(O) - test_output_T\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8, 6])\n",
    "ax.plot(residuals, color='green', alpha=0.2)\n",
    "ax.axhline(0, color='black', linestyle='dashed')\n",
    "plt.xlabel('Output dimension')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.savefig(plot_save_path+f'/residuals.pdf')\n",
    "print(f'Median: {np.median(residuals):.3f} K, Standard deviation: {np.std(residuals):.3f} K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c2ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
