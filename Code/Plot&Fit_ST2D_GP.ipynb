{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing surface temperature map\n",
    "raw_ST_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_ST2D.csv', delimiter=',')\n",
    "#Path to store model\n",
    "model_save_path = base_dir+'Model_Storage/GP_ST/'\n",
    "check_and_make_dir(model_save_path)\n",
    "#Path to store plots\n",
    "plot_save_path = base_dir+'Plots/GP_ST/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_ST_data[:, :4] #has shape 46 x 72 = 3,312\n",
    "raw_outputs = raw_ST_data[:, 5:]\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features\n",
    "O = raw_outputs.shape[1] #Number of outputs\n",
    "\n",
    "## HYPER-PARAMETERS ##\n",
    "#Defining partition of data used for 1. training and 2. testing\n",
    "data_partition = [0.8, 0.2]\n",
    "\n",
    "#Definine sub-partitiion for splitting NN dataset\n",
    "sub_data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 1\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "rng = torch.Generator(device=device)\n",
    "rng.manual_seed(partition_seed)\n",
    "\n",
    "# Variable to show plots or not \n",
    "show_plot = False\n",
    "\n",
    "#Number of nearest neighbors to choose\n",
    "N_neigbors = 500\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-5\n",
    "\n",
    "#Batch size \n",
    "batch_size = 64\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 10\n",
    "\n",
    "#Define storage for losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#Mode for optimization\n",
    "run_mode = 'use'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91916a9",
   "metadata": {},
   "source": [
    "# Plotting of the surface temperature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_input, raw_output in zip(raw_inputs,raw_outputs):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n",
    "    \n",
    "    hm = sns.heatmap(raw_output.reshape((46, 72)), ax=ax)\n",
    "    cbar = hm.collections[0].colorbar\n",
    "    cbar.set_label('Temperature (K)')\n",
    "\n",
    "    # Fix longitude ticks\n",
    "    ax.set_xticks(np.linspace(0, 72, 5))\n",
    "    ax.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "\n",
    "    # Fix latitude ticks\n",
    "    ax.set_yticks(np.linspace(0, 46, 5))\n",
    "    ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "\n",
    "    ax.set_xlabel('Longitude (degrees)')\n",
    "    ax.set_ylabel('Latitude (degrees)')\n",
    "    plt.suptitle(rf'H$_2$ : {raw_input[0]} bar, CO$_2$ : {raw_input[1]} bar, LoD : {raw_input[2]:.0f} days, Obliquity : {raw_input[3]} deg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d894c0",
   "metadata": {},
   "source": [
    "# Fitting data with an Ensemble Conditional GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad32a1a",
   "metadata": {},
   "source": [
    "## First step : partition data into a training set, and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving indices of data partitions\n",
    "train_idx, test_idx = torch.utils.data.random_split(range(N), data_partition, generator=rng)\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = raw_inputs[train_idx]\n",
    "train_outputs = raw_outputs[train_idx]\n",
    "\n",
    "### Testing\n",
    "test_inputs = raw_inputs[test_idx]\n",
    "test_outputs = raw_outputs[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f11a3",
   "metadata": {},
   "source": [
    "## Second step : Building Sai's Conditional GP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ccf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sai_CGP(obs_features, obs_labels, query_features):\n",
    "    \"\"\"\n",
    "    Conditional Gaussian Process\n",
    "    Inputs: \n",
    "        obs_features : ndarray (D, N)\n",
    "            D-dimensional features of the N observation data points.\n",
    "        obs_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the N observation data points.\n",
    "        query_features : ndarray (D, 1)\n",
    "            D-dimensional features of the query data point.\n",
    "    Outputs:\n",
    "        query_labels : ndarray (K, 1)\n",
    "            K-dimensional labels of the query data point.\n",
    "\n",
    "    \"\"\"\n",
    "    # Defining relevant means\n",
    "    mean_obs_labels = np.mean(obs_labels, axis=1, keepdims=True)\n",
    "    mean_obs_features = np.mean(obs_features, axis=1, keepdims=True)\n",
    "    \n",
    "    # Defining relevant covariance matrices\n",
    "    ## Between feature and label of observation data\n",
    "    Cyx = (obs_labels @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and feature of observation data\n",
    "    Cxy = (obs_features @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between feature and feature of observation data\n",
    "    Cxx = (obs_features @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and label of observation data\n",
    "    Cyy = (obs_labels @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Adding regularizer to avoid singularities\n",
    "    Cxx += 1e-8 * np.eye(Cxx.shape[0]) \n",
    "\n",
    "    query_mean_labels = mean_obs_labels + (Cyx @ scipy.linalg.inv(Cxx) @ (query_features - mean_obs_features))\n",
    "\n",
    "    query_cov_labels = Cyy - Cyx @ scipy.linalg.inv(Cxx) @ Cxy\n",
    "\n",
    "    return query_mean_labels, query_cov_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e19df2",
   "metadata": {},
   "source": [
    "## Third step : Going through test set (query points), find observations in proximity, and use them to get guess labels for query point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65772b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "res = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "for query_idx, (test_input, test_output) in enumerate(zip(test_inputs, test_outputs)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    distances = np.sqrt( (test_input[0] - train_inputs[:,0])**2 + (test_input[1] - train_inputs[:,1])**2 + (test_input[2] - train_inputs[:,2])**2 + (test_input[3] - train_inputs[:,3])**2 )\n",
    "\n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs = train_outputs[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, prox_train_outputs.T, test_input.reshape((1, 4)).T)\n",
    "    model_test_output = mean_test_output[:,0] \n",
    "    model_test_output_err = np.sqrt(np.diag(cov_test_output))\n",
    "    res[query_idx, :] = model_test_output - test_output\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cov_test_output, cmap='coolwarm', origin='lower')\n",
    "        plt.colorbar(label='Covariance')\n",
    "        plt.title('Joint Covariance Matrix')\n",
    "        plt.xlabel('Output index')\n",
    "        plt.ylabel('Output index')\n",
    "        plt.show()\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_model_test_output = mean_test_output.reshape((46, 72))\n",
    "        plot_res = res[query_idx, :].reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('Model')\n",
    "        hm2 = sns.heatmap(plot_model_test_output, ax=ax2)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax3.set_title('Residuals')\n",
    "        hm3 = sns.heatmap(plot_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        # Shared colorbar (use the last heatmap's mappable)\n",
    "        # cbar = fig.colorbar(hm3.get_children()[0], ax=[ax1, ax2, ax3], location='right')\n",
    "        # cbar.set_label(\"Temperature\")\n",
    "        # Fix longitude ticks\n",
    "        ax3.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax3.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax3.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$ : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b66de",
   "metadata": {},
   "source": [
    "# Fourth step : Build a encoder-decoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b59b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"Encoder-Decoder architecture for 46x72 image prediction\"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # 46x72 -> 23x36\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # 23x36 -> 11x18\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder with explicit size handling\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # 11x18 -> 22x36\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),  # 128 = 64 (from up1) + 64 (skip)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)  # 22x36 -> 44x72\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),  # 64 = 32 (from up2) + 32 (skip)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final adjustment to get exact 46x72\n",
    "        self.final_up = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder with skip connections\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        bottleneck = self.bottleneck(p2)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d1 = self.up1(bottleneck)\n",
    "        e2_cropped = e2[:, :, :d1.size(2), :d1.size(3)]\n",
    "        d1 = torch.cat([d1, e2_cropped], dim=1)  # Crop e2 if needed\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        e1_cropped = e1[:, :, :d2.size(2), :d2.size(3)]\n",
    "        d2 = torch.cat([d2, e1_cropped], dim=1)  # Crop e1 if needed\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        # Pad to exact size if needed\n",
    "        if d2.size(2) < 46 or d2.size(3) < 72:\n",
    "            pad_h = 46 - d2.size(2)\n",
    "            pad_w = 72 - d2.size(3)\n",
    "            d2 = nn.functional.pad(d2, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "            \n",
    "        # Final adjustment to 46x72\n",
    "        d2 = self.final_up(d2)\n",
    "        output = self.final_conv(d2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_inputs, train_outputs, valid_inputs, valid_outputs, \n",
    "                 test_inputs, test_outputs, batch_size, rng, reshape_for_cnn=False, \n",
    "                 img_channels=1, img_height=None, img_width=None,\n",
    "                 output_channels=None, output_height=None, output_width=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = rng\n",
    "        self.reshape_for_cnn = reshape_for_cnn\n",
    "        self.img_channels = img_channels\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        \n",
    "        # Reshape data if needed for CNN\n",
    "        if reshape_for_cnn:\n",
    "            # Reshape inputs\n",
    "            if img_height is None or img_width is None:\n",
    "                # Auto-calculate square dimensions if not provided\n",
    "                total_features = train_inputs.shape[1]\n",
    "                img_size = int(np.sqrt(total_features / img_channels))\n",
    "                if img_size * img_size * img_channels != total_features:\n",
    "                    raise ValueError(f\"Cannot reshape {total_features} features into square image. \"\n",
    "                                     f\"Please provide img_height and img_width explicitly.\")\n",
    "                self.img_height = img_size\n",
    "                self.img_width = img_size\n",
    "            \n",
    "            self.train_inputs = train_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_inputs = valid_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_inputs = test_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            \n",
    "            self.train_outputs = train_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_outputs = valid_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_outputs = test_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "\n",
    "        else:\n",
    "            self.train_inputs = train_inputs\n",
    "            self.valid_inputs = valid_inputs\n",
    "            self.test_inputs = test_inputs\n",
    "            self.train_outputs = train_outputs\n",
    "            self.valid_outputs = valid_outputs\n",
    "            self.test_outputs = test_outputs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_inputs, self.train_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, generator=self.rng)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.valid_inputs, self.valid_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_inputs, self.test_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70380314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder().to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228ee6b",
   "metadata": {},
   "source": [
    "# Fifth step : Build training dataset for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "train_NN_inputs = np.zeros(train_outputs.shape, dtype=float)\n",
    "\n",
    "for query_idx, (query_input, query_output) in enumerate(zip(train_inputs, train_outputs)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    distances = np.sqrt( (query_input[0] - train_inputs[:,0])**2 + (query_input[1] - train_inputs[:,1])**2 + (query_input[2] - train_inputs[:,2])**2 + (query_input[3] - train_inputs[:,3])**2 )\n",
    "\n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs = train_outputs[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, prox_train_outputs.T, query_input.reshape((1, 4)).T)\n",
    "    model_test_output = mean_test_output[:,0] \n",
    "    model_test_output_err = np.sqrt(np.diag(cov_test_output))\n",
    "    train_NN_inputs[query_idx, :] = model_test_output\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = query_output.reshape((46, 72))\n",
    "        plot_model_test_output = mean_test_output.reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(query_output)\n",
    "        vmax = np.max(query_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('Model')\n",
    "        hm2 = sns.heatmap(plot_model_test_output, ax=ax2)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax2.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax2.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax2.set_xlabel('Longitude (degrees)')\n",
    "        \n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$O : {query_input[0]} bar, CO$_2$ : {query_input[1]} bar, LoD : {query_input[2]:.0f} days, Obliquity : {query_input[3]} deg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset into training, validation, and testing, and format it correctly\n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(train_inputs.shape[0]), sub_data_partitions, generator=rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "NN_train_inputs = torch.tensor(train_NN_inputs[train_idx], dtype=torch.float32)\n",
    "NN_train_outputs = torch.tensor(train_outputs[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "NN_valid_inputs = torch.tensor(train_NN_inputs[valid_idx], dtype=torch.float32)\n",
    "NN_valid_outputs = torch.tensor(train_outputs[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "NN_test_og_inputs = torch.tensor(train_inputs[test_idx], dtype=torch.float32) \n",
    "NN_test_inputs = torch.tensor(train_NN_inputs[test_idx], dtype=torch.float32)\n",
    "NN_test_outputs = torch.tensor(train_outputs[test_idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataModule\n",
    "data_module = CustomDataModule(\n",
    "    NN_train_inputs, NN_train_outputs,\n",
    "    NN_valid_inputs, NN_valid_outputs,\n",
    "    NN_test_inputs, NN_test_outputs,\n",
    "    batch_size, rng, reshape_for_cnn=True,\n",
    "    img_channels=1, img_height=46, img_width=72\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbf238",
   "metadata": {},
   "source": [
    "# Sixth step : Define optimization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class RegressionModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer_class = optimizer\n",
    "        \n",
    "        # Store losses\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.model.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18beb94",
   "metadata": {},
   "source": [
    "# Seventh step : Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "lightning_module = RegressionModule(\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = CSVLogger(model_save_path+'logs', name='NeuralNetwork')\n",
    "\n",
    "# Create Trainer and train\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    logger=logger,\n",
    "    deterministic=True  # For reproducibility\n",
    ")\n",
    "\n",
    "if run_mode == 'use':\n",
    "    \n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    # Save model (PyTorch Lightning style)\n",
    "    trainer.save_checkpoint(model_save_path + f'{n_epochs}epochs_{learning_rate}LR_{batch_size}BS.ckpt')\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "else:\n",
    "    # Load model\n",
    "    lightning_module = RegressionModule.load_from_checkpoint(\n",
    "        model_save_path + f'{n_epochs}epochs_{learning_rate}LR_{batch_size}BS.ckpt',\n",
    "        model=model,\n",
    "        optimizer=SGD,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec704607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model on test dataset\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accessing Training History After Training ---\n",
    "\n",
    "# Find the version directory (e.g., version_0, version_1, etc.)\n",
    "log_dir = model_save_path+'logs/NeuralNetwork'\n",
    "versions = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "latest_version = sorted(versions)[-1]  # Get the latest version\n",
    "csv_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "\n",
    "# Read the metrics\n",
    "metrics_df = pd.read_csv(csv_path)\n",
    "print(metrics_df.head())\n",
    "\n",
    "# Extract losses per epoch\n",
    "train_losses = metrics_df[metrics_df['train_loss_epoch'].notna()]['train_loss_epoch'].tolist()\n",
    "eval_losses = metrics_df[metrics_df['valid_loss'].notna()]['valid_loss'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d2b28",
   "metadata": {},
   "source": [
    "# Eigth step : Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c363214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[3, 1]}, figsize=(10, 6))\n",
    "ax1.plot(np.arange(n_epochs), train_losses, label=\"Train\")\n",
    "ax1.plot(np.arange(n_epochs), eval_losses, label=\"Validation\")\n",
    "ax2.plot(np.arange(n_epochs), np.array(train_losses) - np.array(eval_losses), label=\"Train\")\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax2.set_ylabel(\"Loss Diff.\")\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.savefig(plot_save_path+'/loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing GP predicted ST maps vs NN predicted ST maps vs true ST maps with residuals\n",
    "substep = 1000\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(NN_test_outputs) != np.ndarray):\n",
    "    NN_test_outputs = NN_test_outputs.numpy()\n",
    "\n",
    "GP_res = np.zeros(NN_test_outputs.shape, dtype=float)\n",
    "NN_res = np.zeros(NN_test_outputs.shape, dtype=float)\n",
    "\n",
    "for NN_test_idx, (NN_test_input, GP_test_output, NN_test_output) in enumerate(zip(NN_test_og_inputs, NN_test_inputs, NN_test_outputs)):\n",
    "\n",
    "    #Retrieve prediction\n",
    "    NN_pred_output = model(GP_test_output.reshape(1, 1, 46, 72)).detach().numpy().reshape(3312)\n",
    "\n",
    "    #Convert to numpy\n",
    "    NN_test_input = NN_test_input.numpy()\n",
    "\n",
    "    #Storing residuals \n",
    "    GP_res[NN_test_idx, :] = GP_test_output - NN_test_output\n",
    "    NN_res[NN_test_idx, :] = NN_pred_output - NN_test_output\n",
    "\n",
    "    #Plotting\n",
    "    if (NN_test_idx % substep == 0):\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = NN_test_output.reshape((46, 72))\n",
    "        plot_NN_test_output = NN_pred_output.reshape((46, 72))\n",
    "        plot_GP_test_output = GP_test_output.reshape((46, 72))\n",
    "        plot_res_GP = GP_res[NN_test_idx, :].reshape((46, 72))\n",
    "        plot_res_NN = NN_res[NN_test_idx, :].reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(NN_test_output)\n",
    "        vmax = np.max(NN_test_output)\n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('GP Model')\n",
    "        hm2 = sns.heatmap(plot_GP_test_output, ax=ax2)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax3.set_title('NN Model')\n",
    "        hm3 = sns.heatmap(plot_NN_test_output, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax4.set_title('GP Residuals')\n",
    "        hm4 = sns.heatmap(plot_res_GP, ax=ax4)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm4.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax5.set_title('NN Residuals')\n",
    "        hm5 = sns.heatmap(plot_res_NN, ax=ax5)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm5.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        # Shared colorbar (use the last heatmap's mappable)\n",
    "        # cbar = fig.colorbar(hm3.get_children()[0], ax=[ax1, ax2, ax3], location='right')\n",
    "        # cbar.set_label(\"Temperature\")\n",
    "        # Fix longitude ticks\n",
    "        ax5.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax5.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax5.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$ : {NN_test_input[0]} bar, CO$_2$ : {NN_test_input[1]} bar, LoD : {NN_test_input[2]:.0f} days, Obliquity : {NN_test_input[3]} deg')\n",
    "\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{NN_test_idx}.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b19ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
