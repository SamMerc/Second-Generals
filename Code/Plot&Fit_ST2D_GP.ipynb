{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing surface temperature map\n",
    "raw_ST_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_ST2D.csv', delimiter=',')\n",
    "#Path to store model\n",
    "model_save_path = base_dir+'Model_Storage/GP_ST/'\n",
    "check_and_make_dir(model_save_path)\n",
    "#Path to store plots\n",
    "plot_save_path = base_dir+'Plots/GP_ST/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_ST_data[:, :4] #has shape 46 x 72 = 3,312\n",
    "raw_outputs = raw_ST_data[:, 5:]\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features\n",
    "O = raw_outputs.shape[1] #Number of outputs\n",
    "\n",
    "## HYPER-PARAMETERS ##\n",
    "#Defining partition of data used for 1. training and 2. testing\n",
    "data_partition = [0.8, 0.2]\n",
    "\n",
    "#Definine sub-partitiion for splitting NN dataset\n",
    "sub_data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 1\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "rng = torch.Generator(device=device)\n",
    "rng.manual_seed(partition_seed)\n",
    "\n",
    "# Variable to show plots or not \n",
    "show_plot = False\n",
    "\n",
    "#Number of nearest neighbors to choose\n",
    "N_neigbors = 10\n",
    "\n",
    "#Distance metric to use\n",
    "distance_metric = 'euclidean' #options: 'euclidean', 'mahalanobis', 'logged_euclidean', 'logged_mahalanobis'\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-5\n",
    "\n",
    "#Regularization coefficient\n",
    "regularization_coeff = 1e-2\n",
    "\n",
    "#Weight decay \n",
    "weight_decay = 0.0\n",
    "\n",
    "#Batch size \n",
    "batch_size = 200\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 10\n",
    "\n",
    "#Mode for optimization\n",
    "run_mode = 'use'\n",
    "\n",
    "#Convert raw inputs for H2 and CO2 pressures to log10 scale so don't have to deal with it later\n",
    "if 'logged' in distance_metric:\n",
    "    raw_inputs[:, 0] = np.log10(raw_inputs[:, 0]) #H2\n",
    "    raw_inputs[:, 1] = np.log10(raw_inputs[:, 1]) #CO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91916a9",
   "metadata": {},
   "source": [
    "# Plotting of the surface temperature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_input, raw_output in zip(raw_inputs,raw_outputs):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n",
    "    \n",
    "    hm = sns.heatmap(raw_output.reshape((46, 72)), ax=ax)\n",
    "    cbar = hm.collections[0].colorbar\n",
    "    cbar.set_label('Temperature (K)')\n",
    "\n",
    "    # Fix longitude ticks\n",
    "    ax.set_xticks(np.linspace(0, 72, 5))\n",
    "    ax.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "\n",
    "    # Fix latitude ticks\n",
    "    ax.set_yticks(np.linspace(0, 46, 5))\n",
    "    ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "\n",
    "    ax.set_xlabel('Longitude (degrees)')\n",
    "    ax.set_ylabel('Latitude (degrees)')\n",
    "    plt.suptitle(rf'H$_2$ : {raw_input[0]} bar, CO$_2$ : {raw_input[1]} bar, LoD : {raw_input[2]:.0f} days, Obliquity : {raw_input[3]} deg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d894c0",
   "metadata": {},
   "source": [
    "# Fitting data with an Ensemble Conditional GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad32a1a",
   "metadata": {},
   "source": [
    "## First step : partition data into a training set, and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving indices of data partitions\n",
    "train_idx, test_idx = torch.utils.data.random_split(range(N), data_partition, generator=rng)\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = raw_inputs[train_idx]\n",
    "train_outputs = raw_outputs[train_idx]\n",
    "\n",
    "### Testing\n",
    "test_inputs = raw_inputs[test_idx]\n",
    "test_outputs = raw_outputs[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f11a3",
   "metadata": {},
   "source": [
    "## Second step : Building Sai's Conditional GP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ccf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sai_CGP(obs_features, obs_labels, query_features):\n",
    "    \"\"\"\n",
    "    Conditional Gaussian Process\n",
    "    Inputs: \n",
    "        obs_features : ndarray (D, N)\n",
    "            D-dimensional features of the N ensemble data points.\n",
    "        obs_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the N ensemble data points.\n",
    "        query_features : ndarray (D, 1)\n",
    "            D-dimensional features of the query data point.\n",
    "    Outputs:\n",
    "        query_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the ensemble updated from the query point.\n",
    "        query_cov_labels : ndarray (K, K)\n",
    "            K-by-K covariance of the ensemble labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining relevant covariance matrices\n",
    "    ## Between feature and label of observation data\n",
    "    Cyx = (obs_labels @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and feature of observation data\n",
    "    Cxy = (obs_features @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between feature and feature of observation data\n",
    "    Cxx = (obs_features @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and label of observation data\n",
    "    Cyy = (obs_labels @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Adding regularizer to avoid singularities\n",
    "    Cxx += 1e-8 * np.eye(Cxx.shape[0]) \n",
    "\n",
    "    query_labels = obs_labels + (Cyx @ scipy.linalg.pinv(Cxx) @ (query_features - obs_features))\n",
    "\n",
    "    query_cov_labels = Cyy - Cyx @ scipy.linalg.pinv(Cxx) @ Cxy\n",
    "\n",
    "    return query_labels, query_cov_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e19df2",
   "metadata": {},
   "source": [
    "## Third step : Going through test set (query points), find observations in proximity, and use them to get guess labels for query point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65772b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "res = np.zeros(test_outputs.shape, dtype=float)\n",
    "\n",
    "for query_idx, (test_input, test_output) in enumerate(zip(test_inputs, test_outputs)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    # Euclidian distance\n",
    "    if 'euclidean' in distance_metric:\n",
    "        distances = np.sqrt( (test_input[0] - train_inputs[:,0])**2 + (test_input[1] - train_inputs[:,1])**2 + (test_input[2] - train_inputs[:,2])**2 + (test_input[3] - train_inputs[:,3])**2 )\n",
    "    # Mahalanobis distance\n",
    "    elif 'mahalanobis' in distance_metric:\n",
    "        distances = np.sqrt( (test_input - np.mean(train_inputs, axis=0)).T @ scipy.linalg.inv((train_inputs.T @ train_inputs) / (train_inputs.shape[0] - 1)) @ (test_input - np.mean(train_inputs, axis=0)) )\n",
    "    else:raise('Invalid distance metric')\n",
    "    \n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs = train_outputs[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, prox_train_outputs.T, test_input.reshape((1, 4)).T)\n",
    "    model_test_output = np.mean(mean_test_output,axis=1)\n",
    "    model_test_output_err = np.sqrt(np.diag(cov_test_output))\n",
    "    res[query_idx, :] = model_test_output - test_output\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cov_test_output, cmap='coolwarm', origin='lower')\n",
    "        plt.colorbar(label='Covariance')\n",
    "        plt.title('Joint Covariance Matrix')\n",
    "        plt.xlabel('Output index')\n",
    "        plt.ylabel('Output index')\n",
    "        plt.show()\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = test_output.reshape((46, 72))\n",
    "        plot_model_test_output = model_test_output.reshape((46, 72))\n",
    "        plot_res = res[query_idx, :].reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(test_output)\n",
    "        vmax = np.max(test_output)\n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('Model')\n",
    "        hm2 = sns.heatmap(plot_model_test_output, ax=ax2)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax3.set_title('Residuals')\n",
    "        hm3 = sns.heatmap(plot_res, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        # Shared colorbar (use the last heatmap's mappable)\n",
    "        # cbar = fig.colorbar(hm3.get_children()[0], ax=[ax1, ax2, ax3], location='right')\n",
    "        # cbar.set_label(\"Temperature\")\n",
    "        # Fix longitude ticks\n",
    "        ax3.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax3.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax3.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$ : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b66de",
   "metadata": {},
   "source": [
    "# Fourth step : Build a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b59b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Direct CNN - no dimensionality reduction\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Input: input_channels x 48 x 69\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 32 x 48 x 69\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 64 x 48 x 69\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 64 x 48 x 69\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output: 32 x 48 x 69\n",
    "            \n",
    "            nn.Conv2d(32, output_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "            # Output: output_channels x 48 x 69\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the CNN.\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_channels, 46, 72)\n",
    "        Returns:\n",
    "            Output images of shape (batch_size, output_channels, 46, 72)\n",
    "        \"\"\"\n",
    "        x = self.cnn(x)\n",
    "        return x\n",
    "    \n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_inputs, train_outputs, valid_inputs, valid_outputs, \n",
    "                 test_inputs, test_outputs, batch_size, rng, reshape_for_cnn=False, \n",
    "                 img_channels=1, img_height=None, img_width=None):\n",
    "        super().__init__()\n",
    "\n",
    "        #Store original shapes for reshaping \n",
    "        self.batch_size = batch_size\n",
    "        self.rng = rng\n",
    "        self.reshape_for_cnn = reshape_for_cnn\n",
    "        self.img_channels = img_channels\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        \n",
    "        # Standardizing the output\n",
    "        ## Create scaler\n",
    "        out_scaler = StandardScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        out_scaler.fit(train_outputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_outputs = torch.tensor(out_scaler.transform(train_outputs.numpy()), dtype=torch.float32)\n",
    "        valid_outputs = torch.tensor(out_scaler.transform(valid_outputs.numpy()), dtype=torch.float32)\n",
    "        test_outputs = torch.tensor(out_scaler.transform(test_outputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.out_scaler = out_scaler\n",
    "        \n",
    "        # Normalizing the input\n",
    "        ## Create scaler\n",
    "        in_scaler = MinMaxScaler()\n",
    "        \n",
    "        ## Fit scaler on training dataset (convert to numpy)\n",
    "        in_scaler.fit(train_inputs.numpy())\n",
    "        \n",
    "        ## Transform all datasets and convert back to tensors\n",
    "        train_inputs = torch.tensor(in_scaler.transform(train_inputs.numpy()), dtype=torch.float32)\n",
    "        valid_inputs = torch.tensor(in_scaler.transform(valid_inputs.numpy()), dtype=torch.float32)\n",
    "        test_inputs = torch.tensor(in_scaler.transform(test_inputs.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        # Store the scaler if you need to inverse transform later\n",
    "        self.in_scaler = in_scaler\n",
    "        \n",
    "        #Store the inputs\n",
    "        self.train_inputs = train_inputs\n",
    "        self.valid_inputs = valid_inputs\n",
    "        self.test_inputs = test_inputs\n",
    "\n",
    "        # Reshape data if needed for CNN\n",
    "        if reshape_for_cnn:\n",
    "            # Reshape inputs\n",
    "            if img_height is None or img_width is None:\n",
    "                # Auto-calculate square dimensions if not provided\n",
    "                total_features = train_inputs.shape[1]\n",
    "                img_size = int(np.sqrt(total_features / img_channels))\n",
    "                if img_size * img_size * img_channels != total_features:\n",
    "                    raise ValueError(f\"Cannot reshape {total_features} features into square image. \"\n",
    "                                     f\"Please provide img_height and img_width explicitly.\")\n",
    "                self.img_height = img_size\n",
    "                self.img_width = img_size\n",
    "            \n",
    "            self.train_inputs = train_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_inputs = valid_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_inputs = test_inputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            \n",
    "            self.train_outputs = train_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.valid_outputs = valid_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "            self.test_outputs = test_outputs.reshape(-1, img_channels, self.img_height, self.img_width)\n",
    "\n",
    "        else:\n",
    "            self.train_inputs = train_inputs\n",
    "            self.valid_inputs = valid_inputs\n",
    "            self.test_inputs = test_inputs\n",
    "            self.train_outputs = train_outputs\n",
    "            self.valid_outputs = valid_outputs\n",
    "            self.test_outputs = test_outputs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_inputs, self.train_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, generator=self.rng)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.valid_inputs, self.valid_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_inputs, self.test_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70380314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(1,1).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228ee6b",
   "metadata": {},
   "source": [
    "# Fifth step : Build training dataset for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "train_NN_inputs = np.zeros(train_outputs.shape, dtype=float)\n",
    "\n",
    "for query_idx, (query_input, query_output) in enumerate(zip(train_inputs, train_outputs)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    # Euclidian distance\n",
    "    if 'euclidean' in distance_metric:\n",
    "        distances = np.sqrt( (query_input[0] - train_inputs[:,0])**2 + (query_input[1] - train_inputs[:,1])**2 + (query_input[2] - train_inputs[:,2])**2 + (query_input[3] - train_inputs[:,3])**2 )\n",
    "    # Mahalanobis distance\n",
    "    elif 'mahalanobis' in distance_metric:\n",
    "        distances = np.sqrt( (query_input - np.mean(train_inputs, axis=0)).T @ scipy.linalg.inv((train_inputs @ train_inputs.T) / (train_inputs.shape[0] - 1)) @ (query_input - np.mean(train_inputs, axis=0)) )\n",
    "    else:raise('Invalid distance metric')\n",
    "    \n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs = train_outputs[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, prox_train_outputs.T, query_input.reshape((1, 4)).T)\n",
    "    model_test_output = np.mean(mean_test_output,axis=1)\n",
    "    model_test_output_err = np.sqrt(np.diag(cov_test_output))\n",
    "    train_NN_inputs[query_idx, :] = model_test_output\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = query_output.reshape((46, 72))\n",
    "        plot_model_test_output = model_test_output.reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(query_output)\n",
    "        vmax = np.max(query_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('Model')\n",
    "        hm2 = sns.heatmap(plot_model_test_output, ax=ax2)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "\n",
    "        ax2.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax2.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax2.set_xlabel('Longitude (degrees)')\n",
    "        \n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$O : {query_input[0]} bar, CO$_2$ : {query_input[1]} bar, LoD : {query_input[2]:.0f} days, Obliquity : {query_input[3]} deg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset into training, validation, and testing, and format it correctly\n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(train_inputs.shape[0]), sub_data_partitions, generator=rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "NN_train_inputs = torch.tensor(train_NN_inputs[train_idx], dtype=torch.float32)\n",
    "NN_train_outputs = torch.tensor(train_outputs[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "NN_valid_inputs = torch.tensor(train_NN_inputs[valid_idx], dtype=torch.float32)\n",
    "NN_valid_outputs = torch.tensor(train_outputs[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "NN_test_og_inputs = torch.tensor(train_inputs[test_idx], dtype=torch.float32) \n",
    "NN_test_inputs = torch.tensor(train_NN_inputs[test_idx], dtype=torch.float32)\n",
    "NN_test_outputs = torch.tensor(train_outputs[test_idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataModule\n",
    "data_module = CustomDataModule(\n",
    "    NN_train_inputs, NN_train_outputs,\n",
    "    NN_valid_inputs, NN_valid_outputs,\n",
    "    NN_test_inputs, NN_test_outputs,\n",
    "    batch_size, rng, reshape_for_cnn=True,\n",
    "    img_channels=1, img_height=46, img_width=72\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbf238",
   "metadata": {},
   "source": [
    "# Sixth step : Define optimization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class RegressionModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, learning_rate, weight_decay=0.0, reg_coeff=0.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer_class = optimizer\n",
    "    \n",
    "    def compute_gradient_penalty(self, X):\n",
    "        \"\"\"\n",
    "        Compute the gradient of model output with respect to input.\n",
    "        Returns the L2 norm of the gradients as a regularization term.\n",
    "        \"\"\"\n",
    "        if self.reg_coeff == 0:\n",
    "            return torch.tensor(0., device=self.device)\n",
    "        \n",
    "        # Clone and enable gradient computation for inputs\n",
    "        X_grad = X.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Temporarily enable gradients (needed for validation/test steps)\n",
    "        with torch.enable_grad():\n",
    "            \n",
    "            # Compute output (need to recompute to track gradients w.r.t. X)\n",
    "            output = self.model(X_grad)\n",
    "            \n",
    "            # Compute gradients of output with respect to input\n",
    "            grad_outputs = torch.ones_like(output)\n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=output,\n",
    "                inputs=X_grad,\n",
    "                grad_outputs=grad_outputs,\n",
    "                create_graph=True,  # Keep computation graph for backprop\n",
    "                retain_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Compute L2 norm of gradients (squared)\n",
    "            gradient_penalty = torch.mean(gradients ** 2)\n",
    "        \n",
    "        return self.reg_coeff * gradient_penalty\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Add gradient regularization\n",
    "        grad_penalty = self.compute_gradient_penalty(X)\n",
    "        loss += grad_penalty\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18beb94",
   "metadata": {},
   "source": [
    "# Seventh step : Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "lightning_module = RegressionModule(\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_coeff=regularization_coeff,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = CSVLogger(model_save_path+'logs', name='NeuralNetwork')\n",
    "\n",
    "# Create Trainer and train\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    logger=logger,\n",
    "    deterministic=True  # For reproducibility\n",
    ")\n",
    "\n",
    "if run_mode == 'use':\n",
    "    \n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    # Save model (PyTorch Lightning style)\n",
    "    trainer.save_checkpoint(model_save_path + f'{n_epochs}epochs_{regularization_coeff}WD_{regularization_coeff}RC_{learning_rate}LR_{batch_size}BS.ckpt')\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "else:\n",
    "    # Load model\n",
    "    lightning_module = RegressionModule.load_from_checkpoint(\n",
    "        model_save_path + f'{n_epochs}epochs_{regularization_coeff}WD_{regularization_coeff}RC_{learning_rate}LR_{batch_size}BS.ckpt',\n",
    "        model=model,\n",
    "        optimizer=SGD,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_coeff=regularization_coeff,\n",
    "    weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec704607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model on test dataset\n",
    "if run_mode == 'use':trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accessing Training History After Training ---\n",
    "# Find the version directory (e.g., version_0, version_1, etc.)\n",
    "log_dir = model_save_path+'logs/NeuralNetwork'\n",
    "versions = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "latest_version = sorted(versions)[-1]  # Get the latest version\n",
    "csv_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "\n",
    "# Read the metrics\n",
    "metrics_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract losses per epoch\n",
    "train_losses = metrics_df[metrics_df['train_loss_epoch'].notna()]['train_loss_epoch'].tolist()\n",
    "eval_losses = metrics_df[metrics_df['valid_loss'].notna()]['valid_loss'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d2b28",
   "metadata": {},
   "source": [
    "# Eigth step : Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c363214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[3, 1]}, figsize=(10, 6))\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "n_batches = len(train_losses) // n_epochs\n",
    "\n",
    "# Create x-axis in terms of epochs (0 to n_epochs)\n",
    "x_all = np.linspace(0, n_epochs, len(train_losses))\n",
    "x_epoch = np.arange(n_epochs+1)\n",
    "\n",
    "# Plot transparent background showing all batch losses\n",
    "ax1.plot(x_all, train_losses, alpha=0.3, color='C0', linewidth=0.5)\n",
    "ax1.plot(x_all, eval_losses, alpha=0.3, color='C1', linewidth=0.5)\n",
    "\n",
    "# Plot solid lines showing epoch-level losses (every n_batches steps)\n",
    "train_epoch = [train_losses[0]] + train_losses[n_batches-1::n_batches]  # Last batch of each epoch\n",
    "eval_epoch = [eval_losses[0]] + eval_losses[n_batches-1::n_batches]\n",
    "ax1.plot(x_epoch, train_epoch, label=\"Train\", color='C0', linewidth=2, marker='o')\n",
    "ax1.plot(x_epoch, eval_epoch, label=\"Validation\", color='C1', linewidth=2, marker='o')\n",
    "\n",
    "# Same for difference plot\n",
    "diff_all = np.array(train_losses) - np.array(eval_losses)\n",
    "diff_epoch = np.array(train_epoch) - np.array(eval_epoch)\n",
    "\n",
    "ax2.plot(x_all, diff_all, alpha=0.3, color='C2', linewidth=0.5)\n",
    "ax2.plot(x_epoch, diff_epoch, color='C2', linewidth=2, marker='o')\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax2.set_ylabel(\"Loss Diff.\")\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.savefig(plot_save_path+'/loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing GP predicted ST maps vs NN predicted ST maps vs true ST maps with residuals\n",
    "substep = 100\n",
    "\n",
    "# Get the scalers from data module\n",
    "out_scaler = data_module.out_scaler\n",
    "in_scaler = data_module.in_scaler\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(NN_test_outputs) != np.ndarray):\n",
    "    NN_test_outputs = NN_test_outputs.numpy()\n",
    "\n",
    "GP_res = np.zeros(NN_test_outputs.shape, dtype=float)\n",
    "NN_res = np.zeros(NN_test_outputs.shape, dtype=float)\n",
    "\n",
    "for NN_test_idx, (NN_test_input, GP_test_output, NN_test_output) in enumerate(zip(NN_test_og_inputs, NN_test_inputs, NN_test_outputs)):\n",
    "\n",
    "    # Flatten to 2D for the scaler: (1 sample, 3312 features)\n",
    "    GP_test_output_np = GP_test_output.numpy().reshape(1, -1)\n",
    "\n",
    "    # Scale the input\n",
    "    scaled_input = in_scaler.transform(GP_test_output_np)\n",
    "\n",
    "    # If your model expects 4D input, reshape back\n",
    "    # Otherwise, keep it as 2D if that's what the model expects\n",
    "    model_input = torch.tensor(scaled_input, dtype=torch.float32).reshape(1, 1, 46, 72)\n",
    "\n",
    "    # Get prediction\n",
    "    NN_pred_output_scaled = model(model_input).detach().numpy().reshape(3312)\n",
    "\n",
    "    # Inverse transform to get original scale\n",
    "    NN_pred_output = out_scaler.inverse_transform(NN_pred_output_scaled.reshape(1, -1)).flatten()\n",
    "\n",
    "    #Convert to numpy\n",
    "    NN_test_input = NN_test_input.numpy()\n",
    "\n",
    "    #Storing residuals \n",
    "    GP_res[NN_test_idx, :] = GP_test_output - NN_test_output\n",
    "    NN_res[NN_test_idx, :] = NN_pred_output - NN_test_output\n",
    "\n",
    "    #Plotting\n",
    "    if (NN_test_idx % substep == 0):\n",
    "\n",
    "        #Convert shape\n",
    "        plot_test_output = NN_test_output.reshape((46, 72))\n",
    "        plot_NN_test_output = NN_pred_output.reshape((46, 72))\n",
    "        plot_GP_test_output = GP_test_output.reshape((46, 72))\n",
    "        plot_res_GP = GP_res[NN_test_idx, :].reshape((46, 72))\n",
    "        plot_res_NN = NN_res[NN_test_idx, :].reshape((46, 72))\n",
    "        \n",
    "        fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5, 1, figsize=(8, 8), sharex=True, layout='constrained')        \n",
    "        \n",
    "        # Compute global vmin/vmax across all datasets\n",
    "        vmin = np.min(NN_test_output)\n",
    "        vmax = np.max(NN_test_output)\n",
    "        \n",
    "        # Plot heatmaps\n",
    "        ax1.set_title('Data')\n",
    "        hm1 = sns.heatmap(plot_test_output, ax=ax1)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm1.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax2.set_title('GP Model')\n",
    "        hm2 = sns.heatmap(plot_GP_test_output, ax=ax2)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm2.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax3.set_title('NN Model')\n",
    "        hm3 = sns.heatmap(plot_NN_test_output, ax=ax3)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm3.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax4.set_title('GP Residuals')\n",
    "        hm4 = sns.heatmap(plot_res_GP, ax=ax4)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm4.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        ax5.set_title('NN Residuals')\n",
    "        hm5 = sns.heatmap(plot_res_NN, ax=ax5)#, cbar=False, vmin=vmin, vmax=vmax)\n",
    "        cbar = hm5.collections[0].colorbar\n",
    "        cbar.set_label('Temperature (K)')\n",
    "        # Shared colorbar (use the last heatmap's mappable)\n",
    "        # cbar = fig.colorbar(hm3.get_children()[0], ax=[ax1, ax2, ax3], location='right')\n",
    "        # cbar.set_label(\"Temperature\")\n",
    "        # Fix longitude ticks\n",
    "        ax5.set_xticks(np.linspace(0, 72, 5))\n",
    "        ax5.set_xticklabels(np.linspace(-180, 180, 5).astype(int))\n",
    "        ax5.set_xlabel('Longitude (degrees)')\n",
    "        # Fix latitude ticks\n",
    "        for ax in [ax1, ax2, ax3, ax4, ax5]:\n",
    "            ax.set_yticks(np.linspace(0, 46, 5))\n",
    "            ax.set_yticklabels(np.linspace(-90, 90, 5).astype(int))\n",
    "            ax.set_ylabel('Latitude (degrees)')\n",
    "        plt.suptitle(rf'H$_2$ : {NN_test_input[0]} bar, CO$_2$ : {NN_test_input[1]} bar, LoD : {NN_test_input[2]:.0f} days, Obliquity : {NN_test_input[3]} deg')\n",
    "\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{NN_test_idx}.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba390c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
