{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5a7ebf",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import celerite2\n",
    "from celerite2 import terms\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy\n",
    "from torchinfo import summary\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0073caf",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to check if directory exists, if not it generates it\n",
    "def check_and_make_dir(dir):\n",
    "    if not os.path.isdir(dir):os.mkdir(dir)\n",
    "#Base directory \n",
    "base_dir = '/Users/samsonmercier/Desktop/Work/PhD/Research/Second_Generals/'\n",
    "#File containing temperature values\n",
    "raw_T_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_T.csv', delimiter=',')\n",
    "#File containing pressure values\n",
    "raw_P_data = np.loadtxt(base_dir+'Data/bt-4500k/training_data_P.csv', delimiter=',')\n",
    "#Path to store model\n",
    "model_save_path = base_dir+'Model_Storage/GP_server/'\n",
    "check_and_make_dir(model_save_path)\n",
    "#Path to store plots\n",
    "plot_save_path = base_dir+'Plots/GP_server/'\n",
    "check_and_make_dir(plot_save_path)\n",
    "\n",
    "#Last 51 columns are the temperature/pressure values, \n",
    "#First 5 are the input values (H2 pressure in bar, CO2 pressure in bar, LoD in hours, Obliquity in deg, H2+Co2 pressure) but we remove the last one since it's not adding info.\n",
    "raw_inputs = raw_T_data[:, :4]\n",
    "raw_outputs_T = raw_T_data[:, 5:]\n",
    "raw_outputs_P = raw_P_data[:, 5:]\n",
    "#Convert raw outputs to log10 scale so we don't have to deal with it later\n",
    "raw_outputs_P = np.log10(raw_outputs_P/1000)\n",
    "\n",
    "#Storing useful quantitites\n",
    "N = raw_inputs.shape[0] #Number of data points\n",
    "D = raw_inputs.shape[1] #Number of features\n",
    "O = raw_outputs_T.shape[1] #Number of outputs\n",
    "\n",
    "## HYPER-PARAMETERS ##\n",
    "#Defining partition of data used for 1. training and 2. testing\n",
    "data_partition = [0.8, 0.2]\n",
    "\n",
    "#Definine sub-partitiion for splitting NN dataset\n",
    "sub_data_partitions = [0.7, 0.1, 0.2]\n",
    "\n",
    "#Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_threads = 1\n",
    "torch.set_num_threads(num_threads)\n",
    "print(f\"Using {device} device with {num_threads} threads\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "#Defining the noise seed for the random partitioning of the training data\n",
    "partition_seed = 4\n",
    "rng = torch.Generator(device=device)\n",
    "rng.manual_seed(partition_seed)\n",
    "\n",
    "# Variable to show plots or not \n",
    "show_plot = False\n",
    "\n",
    "#Number of nearest neighbors to choose\n",
    "N_neigbors = 500\n",
    "\n",
    "#Neural network width and depth\n",
    "nn_width = 200\n",
    "nn_depth = 8\n",
    "\n",
    "#Optimizer learning rate\n",
    "learning_rate = 1e-5\n",
    "\n",
    "#Batch size \n",
    "batch_size = 64\n",
    "\n",
    "#Number of epochs \n",
    "n_epochs = 100000\n",
    "\n",
    "#Define storage for losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "#Mode for optimization\n",
    "run_mode = 'reuse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91916a9",
   "metadata": {},
   "source": [
    "# Plotting of the T-P profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_input, raw_output_T, raw_output_P in zip(raw_inputs,raw_outputs_T,raw_outputs_P):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n",
    "    ax.plot(raw_output_T, raw_output_P, color='blue', linewidth=2)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Temperature (K)')\n",
    "    ax.set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "    ax.set_title(rf'H$_2$ : {raw_input[0]} bar, CO$_2$ : {raw_input[1]} bar, LoD : {raw_input[2]:.0f} days, Obliquity : {raw_input[3]} deg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d210a",
   "metadata": {},
   "source": [
    "# Fitting data with a Gaussian Process (celerite) - trying it out on one T-P profile (Can't be generalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c057da",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 4\n",
    "\n",
    "#Plot the T-P profile we want to look at\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=[8, 6], gridspec_kw={'height_ratios':[3,1]})\n",
    "ax1.plot(np.log(raw_outputs_P[4]/1000), raw_outputs_T[4], '.', color='blue', linewidth=2, label='Data')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_ylabel('Temperature (K)')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_xlabel(r'log$_{10}$ Pressure (bar)')\n",
    "ax1.set_title(rf'H$_2$O : {raw_inputs[key][0]} bar, CO$_2$ : {raw_inputs[key][1]} bar, LoD : {raw_inputs[key][2]:.0f} days, Obliquity : {raw_inputs[key][3]} deg')\n",
    "\n",
    "#GP\n",
    "#Defining a quasi-periodic term\n",
    "term1 = terms.SHOTerm(sigma=1.0, rho=1.0, tau=10.0)\n",
    "\n",
    "#Defining a non-periodic term\n",
    "term2 = terms.SHOTerm(sigma=1.0, rho=5.0, Q=0.25)\n",
    "kernel = term1 + term2\n",
    "\n",
    "# Setup the GP\n",
    "gp = celerite2.GaussianProcess(kernel, mean=0.0)\n",
    "gp.compute(np.log(raw_outputs_P[4]/1000))\n",
    "\n",
    "#Plot resulting GP fit\n",
    "pred_T, variance = gp.predict(raw_outputs_T[4], t=np.log(raw_outputs_P[4]/1000), return_var=True)\n",
    "sigma = np.sqrt(variance)\n",
    "ax1.plot(np.log(raw_outputs_P[4]/1000), pred_T, label='initial guess')\n",
    "ax1.fill_between(np.log(raw_outputs_P[4]/1000), pred_T - sigma, pred_T + sigma, color=\"C0\", alpha=0.2)\n",
    "ax2.plot(np.log(raw_outputs_P[4]/1000), raw_outputs_T[4]-pred_T)\n",
    "ax2.axhline(0, color='black', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d894c0",
   "metadata": {},
   "source": [
    "# Fitting data with an Ensemble Conditional GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad32a1a",
   "metadata": {},
   "source": [
    "## First step : partition data into a training set, and a testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving indices of data partitions\n",
    "train_idx, test_idx = torch.utils.data.random_split(range(N), data_partition, generator=rng)\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "train_inputs = raw_inputs[train_idx]\n",
    "train_outputs_T = raw_outputs_T[train_idx]\n",
    "train_outputs_P = raw_outputs_P[train_idx]\n",
    "\n",
    "### Testing\n",
    "test_inputs = raw_inputs[test_idx]\n",
    "test_outputs_T = raw_outputs_T[test_idx]\n",
    "test_outputs_P = raw_outputs_P[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f11a3",
   "metadata": {},
   "source": [
    "## Second step : Building Sai's Conditional GP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ccf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sai_CGP(obs_features, obs_labels, query_features):\n",
    "    \"\"\"\n",
    "    Conditional Gaussian Process\n",
    "    Inputs: \n",
    "        obs_features : ndarray (D, N)\n",
    "            D-dimensional features of the N observation data points.\n",
    "        obs_labels : ndarray (K, N)\n",
    "            K-dimensional labels of the N observation data points.\n",
    "        query_features : ndarray (D, 1)\n",
    "            D-dimensional features of the query data point.\n",
    "    Outputs:\n",
    "        query_labels : ndarray (K, 1)\n",
    "            K-dimensional labels of the query data point.\n",
    "\n",
    "    \"\"\"\n",
    "    # Defining relevant means\n",
    "    mean_obs_labels = np.mean(obs_labels, axis=1, keepdims=True)\n",
    "    mean_obs_features = np.mean(obs_features, axis=1, keepdims=True)\n",
    "    \n",
    "    # Defining relevant covariance matrices\n",
    "    ## Between feature and label of observation data\n",
    "    Cyx = (obs_labels @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and feature of observation data\n",
    "    Cxy = (obs_features @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between feature and feature of observation data\n",
    "    Cxx = (obs_features @ obs_features.T) / (obs_features.shape[0] - 1)\n",
    "    ## Between label and label of observation data\n",
    "    Cyy = (obs_labels @ obs_labels.T) / (obs_features.shape[0] - 1)\n",
    "    ## Adding regularizer to avoid singularities\n",
    "    Cxx += 1e-8 * np.eye(Cxx.shape[0]) \n",
    "\n",
    "    query_mean_labels = mean_obs_labels + (Cyx @ scipy.linalg.inv(Cxx) @ (query_features - mean_obs_features))\n",
    "\n",
    "    query_cov_labels = Cyy - Cyx @ scipy.linalg.inv(Cxx) @ Cxy\n",
    "\n",
    "    return query_mean_labels, query_cov_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e19df2",
   "metadata": {},
   "source": [
    "## Third step : Going through test set (query points), find observations in proximity, and use them to get guess labels for query point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65772b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "input_output_residuals_T = np.zeros(test_outputs_T.shape, dtype=float)\n",
    "input_output_residuals_P = np.zeros(test_outputs_P.shape, dtype=float)\n",
    "\n",
    "for query_idx, (test_input, test_output_T, test_output_P) in enumerate(zip(test_inputs, test_outputs_T, test_outputs_P)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    distances = np.sqrt( (test_input[0] - train_inputs[:,0])**2 + (test_input[1] - train_inputs[:,1])**2 + (test_input[2] - train_inputs[:,2])**2 + (test_input[3] - train_inputs[:,3])**2 )\n",
    "\n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs_T = train_outputs_T[N_closest_idx, :]\n",
    "    prox_train_outputs_P = train_outputs_P[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, np.concat((prox_train_outputs_T, prox_train_outputs_P), axis=1).T, test_input.reshape((1, 4)).T)\n",
    "    model_test_output_T = mean_test_output[:O,0] \n",
    "    model_test_output_P = mean_test_output[O:,0] \n",
    "    model_test_output_Terr = np.sqrt(np.diag(cov_test_output))[:O]\n",
    "    model_test_output_Perr = np.sqrt(np.diag(cov_test_output))[O:]\n",
    "    input_output_residuals_T[query_idx, :] = model_test_output_T - test_output_T\n",
    "    input_output_residuals_P[query_idx, :] = model_test_output_P - test_output_P\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        #Plot TP profiles\n",
    "        fig, axs = plt.subplot_mosaic([['res_pressure', '.'],\n",
    "                                       ['results', 'res_temperature']],\n",
    "                              figsize=(8, 6),\n",
    "                              width_ratios=(3, 1), height_ratios=(1, 3),\n",
    "                              layout='constrained')\n",
    "        for prox_idx in range(N_neigbors):\n",
    "            axs['results'].plot(prox_train_outputs_T[prox_idx], prox_train_outputs_P[prox_idx], '.', linestyle='-', color='red', alpha=0.1, linewidth=2, zorder=1, label='Ensemble' if prox_idx==0 else None)\n",
    "        axs['results'].plot(model_test_output_T, model_test_output_P, '.', linestyle='-', color='green', linewidth=2, markersize=10, zorder=2, label='Prediction')\n",
    "        axs['results'].errorbar(model_test_output_T, model_test_output_P, xerr=model_test_output_Terr, yerr=model_test_output_Perr, fmt='.', linestyle='-', color='green', linewidth=2, zorder=2, alpha=0.5, markersize=10)\n",
    "        axs['results'].plot(test_output_T, test_output_P, '.', linestyle='-', color='blue', linewidth=2, zorder=2, markersize=10, label='Truth')\n",
    "        axs['results'].invert_yaxis()\n",
    "        axs['results'].set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "        axs['results'].set_xlabel('Temperature (K)')\n",
    "        axs['results'].grid()\n",
    "        axs['results'].legend()        \n",
    "        \n",
    "        axs['res_temperature'].fill_betweenx(test_output_P, input_output_residuals_T[query_idx, :] - model_test_output_Terr, input_output_residuals_T[query_idx, :] + model_test_output_Terr, color='green', alpha=0.4)\n",
    "        axs['res_temperature'].plot(input_output_residuals_T[query_idx, :], test_output_P, '.', linestyle='-', color='green', linewidth=2)\n",
    "        axs['res_temperature'].axvline(0, color='black', linestyle='dashed', zorder=2)\n",
    "        axs['res_temperature'].invert_yaxis()\n",
    "        axs['res_temperature'].set_xlabel('Residuals (K)')\n",
    "        axs['res_temperature'].yaxis.tick_right()\n",
    "        axs['res_temperature'].yaxis.set_label_position(\"right\")\n",
    "        axs['res_temperature'].grid()\n",
    "\n",
    "        axs['res_pressure'].fill_between(test_output_T, input_output_residuals_P[query_idx, :] - model_test_output_Perr, input_output_residuals_P[query_idx, :] + model_test_output_Perr, color='green', alpha=0.4)\n",
    "        axs['res_pressure'].axhline(0, color='black', linestyle='dashed', zorder=2)\n",
    "        axs['res_pressure'].invert_yaxis()\n",
    "        axs['res_pressure'].set_ylabel('Residuals (bar)')\n",
    "        axs['res_pressure'].xaxis.tick_top()\n",
    "        axs['res_pressure'].xaxis.set_label_position(\"top\")\n",
    "        axs['res_pressure'].grid()\n",
    "\n",
    "        plt.suptitle(rf'H$_2$ : {test_input[0]} bar, CO$_2$ : {test_input[1]} bar, LoD : {test_input[2]:.0f} days, Obliquity : {test_input[3]} deg')\n",
    "        plt.subplots_adjust(hspace=0, wspace=0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Temperature Residuals : Median = {np.median(input_output_residuals_T):.2f} K, Std = {np.std(input_output_residuals_T):.2f} K')\n",
    "print(rf'Pressure Residuals : Median = {np.median(input_output_residuals_P):.9} $log_{10}$ bar, Std = {np.std(input_output_residuals_P):.9} $log_{10}$ bar')\n",
    "\n",
    "#Plot residuals\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=[10, 6])\n",
    "ax1.plot(input_output_residuals_T.T, alpha=0.1, color='green')\n",
    "ax2.plot(input_output_residuals_P.T, alpha=0.1, color='green')\n",
    "for ax in [ax1, ax2]:ax.axhline(0, color='black', linestyle='dashed')\n",
    "ax2.set_xlabel('Index')\n",
    "ax1.set_ylabel('Temperature')\n",
    "ax2.set_ylabel('log$_{10}$ Pressure (bar)')\n",
    "for ax in [ax1, ax2]:ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987bc504",
   "metadata": {},
   "source": [
    "# Fourth step : Build a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0377013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        # Hidden layers\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        # Pack all layers into a Sequential container\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "# PyTorch Lightning DataModule\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_inputs, train_outputs, valid_inputs, valid_outputs, test_inputs, test_outputs, batch_size, rng):\n",
    "        super().__init__()\n",
    "        self.train_inputs = train_inputs\n",
    "        self.train_outputs = train_outputs\n",
    "        self.valid_inputs = valid_inputs\n",
    "        self.valid_outputs = valid_outputs\n",
    "        self.test_inputs = test_inputs\n",
    "        self.test_outputs = test_outputs\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = rng\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TensorDataset(self.train_inputs, self.train_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, generator=self.rng)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = TensorDataset(self.valid_inputs, self.valid_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TensorDataset(self.test_inputs, self.test_outputs)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, generator=self.rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf22ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(2*O, nn_width, 2*O, nn_depth).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f67369",
   "metadata": {},
   "source": [
    "# Fifth step : Build training dataset for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array to store residuals\n",
    "train_NN_inputs_T = np.zeros(train_outputs_T.shape, dtype=float)\n",
    "train_NN_inputs_P = np.zeros(train_outputs_P.shape, dtype=float)\n",
    "\n",
    "for query_idx, (query_input, query_output_T, query_output_P) in enumerate(zip(train_inputs, train_outputs_T, train_outputs_P)):\n",
    "\n",
    "    #Calculate proximity of query point to observations\n",
    "    distances = np.sqrt( (query_input[0] - train_inputs[:,0])**2 + (query_input[1] - train_inputs[:,1])**2 + (query_input[2] - train_inputs[:,2])**2 + (query_input[3] - train_inputs[:,3])**2 )\n",
    "\n",
    "    #Choose the N closest points\n",
    "    N_closest_idx = np.argsort(distances)[:N_neigbors]\n",
    "    prox_train_inputs = train_inputs[N_closest_idx, :]\n",
    "    prox_train_outputs_T = train_outputs_T[N_closest_idx, :]\n",
    "    prox_train_outputs_P = train_outputs_P[N_closest_idx, :]\n",
    "    \n",
    "    #Find the query labels from nearest neigbours\n",
    "    mean_test_output, cov_test_output = Sai_CGP(prox_train_inputs.T, np.concat((prox_train_outputs_T, prox_train_outputs_P), axis=1).T, query_input.reshape((1, 4)).T)\n",
    "    model_test_output_T = mean_test_output[:O,0] \n",
    "    model_test_output_P = mean_test_output[O:,0] \n",
    "    model_test_output_Terr = np.sqrt(np.diag(cov_test_output))[:O]\n",
    "    model_test_output_Perr = np.sqrt(np.diag(cov_test_output))[O:]\n",
    "    train_NN_inputs_T[query_idx, :] = model_test_output_T\n",
    "    train_NN_inputs_P[query_idx, :] = model_test_output_P\n",
    "\n",
    "    #Diagnostic plot\n",
    "    if show_plot:\n",
    "\n",
    "        #Plot TP profiles\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for prox_idx in range(N_neigbors):\n",
    "            ax.plot(prox_train_outputs_T[prox_idx], prox_train_outputs_P[prox_idx], '.', linestyle='-', color='red', alpha=0.1, linewidth=2, zorder=1, label='Ensemble' if prox_idx==0 else None)\n",
    "        ax.plot(model_test_output_T, model_test_output_P, '.', linestyle='-', color='green', linewidth=2, markersize=10, zorder=2, label='Prediction')\n",
    "        ax.plot(query_output_T, query_output_P, '.', linestyle='-', color='blue', linewidth=2, zorder=2, markersize=10, label='Truth')\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "        ax.set_xlabel('Temperature (K)')\n",
    "        ax.grid()\n",
    "        ax.legend()        \n",
    "\n",
    "        plt.suptitle(rf'H$_2$ : {query_input[0]} bar, CO$_2$ : {query_input[1]} bar, LoD : {query_input[2]:.0f} days, Obliquity : {query_input[3]} deg')\n",
    "        plt.subplots_adjust(hspace=0, wspace=0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad01eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training dataset into training, validation, and testing, and format it correctly\n",
    "\n",
    "## Retrieving indices of data partitions\n",
    "train_idx, valid_idx, test_idx = torch.utils.data.random_split(range(train_inputs.shape[0]), sub_data_partitions, generator=rng)\n",
    "\n",
    "## Generate the data partitions\n",
    "### Training\n",
    "NN_train_inputs_T = torch.tensor(train_NN_inputs_T[train_idx], dtype=torch.float32)\n",
    "NN_train_inputs_P = torch.tensor(train_NN_inputs_P[train_idx], dtype=torch.float32)\n",
    "NN_train_outputs_T = torch.tensor(train_outputs_T[train_idx], dtype=torch.float32)\n",
    "NN_train_outputs_P = torch.tensor(train_outputs_P[train_idx], dtype=torch.float32)\n",
    "### Validation\n",
    "NN_valid_inputs_T = torch.tensor(train_NN_inputs_T[valid_idx], dtype=torch.float32)\n",
    "NN_valid_inputs_P = torch.tensor(train_NN_inputs_P[valid_idx], dtype=torch.float32)\n",
    "NN_valid_outputs_T = torch.tensor(train_outputs_T[valid_idx], dtype=torch.float32)\n",
    "NN_valid_outputs_P = torch.tensor(train_outputs_P[valid_idx], dtype=torch.float32)\n",
    "### Testing\n",
    "NN_test_og_inputs = torch.tensor(train_inputs[test_idx], dtype=torch.float32) \n",
    "NN_test_inputs_T = torch.tensor(train_NN_inputs_T[test_idx], dtype=torch.float32)\n",
    "NN_test_inputs_P = torch.tensor(train_NN_inputs_P[test_idx], dtype=torch.float32)\n",
    "NN_test_outputs_T = torch.tensor(train_outputs_T[test_idx], dtype=torch.float32)\n",
    "NN_test_outputs_P = torch.tensor(train_outputs_P[test_idx], dtype=torch.float32)\n",
    "\n",
    "## Concatenating inputs and outputs\n",
    "NN_train_inputs = torch.cat([\n",
    "    NN_train_inputs_T,\n",
    "    NN_train_inputs_P\n",
    "], dim=1)\n",
    "NN_train_outputs = torch.cat([\n",
    "    NN_train_outputs_T,\n",
    "    NN_train_outputs_P\n",
    "], dim=1)\n",
    "\n",
    "NN_valid_inputs = torch.cat([\n",
    "    NN_valid_inputs_T,\n",
    "    NN_valid_inputs_P\n",
    "], dim=1)\n",
    "NN_valid_outputs = torch.cat([\n",
    "    NN_valid_outputs_T,\n",
    "    NN_valid_outputs_P\n",
    "], dim=1)\n",
    "\n",
    "NN_test_inputs = torch.cat([\n",
    "    NN_test_inputs_T,\n",
    "    NN_test_inputs_P\n",
    "], dim=1)\n",
    "NN_test_outputs = torch.cat([\n",
    "    NN_test_outputs_T,\n",
    "    NN_test_outputs_P\n",
    "], dim=1)\n",
    "\n",
    "# Create DataModule\n",
    "data_module = CustomDataModule(\n",
    "    NN_train_inputs, NN_train_outputs,\n",
    "    NN_valid_inputs, NN_valid_outputs,\n",
    "    NN_test_inputs, NN_test_outputs,\n",
    "    batch_size, rng\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6df18",
   "metadata": {},
   "source": [
    "# Sixth step : Define optimization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Module\n",
    "class RegressionModule(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer_class = optimizer\n",
    "        \n",
    "        # Store losses\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.model.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a34f3",
   "metadata": {},
   "source": [
    "# Seventh step : Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lightning Module\n",
    "lightning_module = RegressionModule(\n",
    "    model=model,\n",
    "    optimizer=SGD,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "logger = CSVLogger(model_save_path+'logs', name='NeuralNetwork')\n",
    "\n",
    "# Create Trainer and train\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    logger=logger,\n",
    "    deterministic=True  # For reproducibility\n",
    ")\n",
    "\n",
    "if run_mode == 'use':\n",
    "    \n",
    "    trainer.fit(lightning_module, datamodule=data_module)\n",
    "    \n",
    "    # Save model (PyTorch Lightning style)\n",
    "    trainer.save_checkpoint(model_save_path + f'{n_epochs}epochs_{learning_rate}LR_{batch_size}BS.ckpt')\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "else:\n",
    "    # Load model\n",
    "    lightning_module = RegressionModule.load_from_checkpoint(\n",
    "        model_save_path + f'{n_epochs}epochs_{learning_rate}LR_{batch_size}BS.ckpt',\n",
    "        model=model,\n",
    "        optimizer=SGD,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be958eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model on test dataset\n",
    "trainer.test(lightning_module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Accessing Training History After Training ---\n",
    "\n",
    "# Find the version directory (e.g., version_0, version_1, etc.)\n",
    "log_dir = model_save_path+'logs/NeuralNetwork'\n",
    "versions = [d for d in os.listdir(log_dir) if d.startswith('version_')]\n",
    "latest_version = sorted(versions)[-1]  # Get the latest version\n",
    "csv_path = os.path.join(log_dir, latest_version, 'metrics.csv')\n",
    "\n",
    "# Read the metrics\n",
    "metrics_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract losses per epoch\n",
    "train_losses = metrics_df[metrics_df['train_loss_epoch'].notna()]['train_loss_epoch'].tolist()\n",
    "eval_losses = metrics_df[metrics_df['valid_loss'].notna()]['valid_loss'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b449f",
   "metadata": {},
   "source": [
    "# Eigth step : Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw={'height_ratios':[3, 1]}, figsize=(10, 6))\n",
    "ax1.plot(np.arange(n_epochs), train_losses, label=\"Train\")\n",
    "ax1.plot(np.arange(n_epochs), eval_losses, label=\"Validation\")\n",
    "ax2.plot(np.arange(n_epochs), np.array(train_losses) - np.array(eval_losses), label=\"Train\")\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"MSE Loss\")\n",
    "ax2.set_ylabel(\"Loss Diff.\")\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.savefig(plot_save_path+'/loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9396807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing GP predicted T-P profiles vs NN predicted T-P profiles vs true T-P profiles with residuals\n",
    "substep = 100\n",
    "\n",
    "#Converting tensors to numpy arrays if this isn't already done\n",
    "if (type(NN_test_outputs_T) != np.ndarray):\n",
    "    NN_test_outputs_T = NN_test_outputs_T.numpy()\n",
    "    NN_test_outputs_P = NN_test_outputs_P.numpy()\n",
    "\n",
    "GP_res_T = np.zeros(NN_test_outputs_P.shape, dtype=float)\n",
    "GP_res_P = np.zeros(NN_test_outputs_P.shape, dtype=float)\n",
    "NN_res_T = np.zeros(NN_test_outputs_P.shape, dtype=float)\n",
    "NN_res_P = np.zeros(NN_test_outputs_P.shape, dtype=float)\n",
    "\n",
    "for NN_test_idx, (NN_test_input, GP_test_output_T, GP_test_output_P, NN_test_output_T, NN_test_output_P) in enumerate(zip(NN_test_og_inputs, NN_test_inputs_T, NN_test_inputs_P, NN_test_outputs_T, NN_test_outputs_P)):\n",
    "\n",
    "    #Retrieve prediction\n",
    "    NN_pred_output = model(torch.cat([GP_test_output_T,GP_test_output_P])).detach().numpy()\n",
    "    NN_pred_output_T = NN_pred_output[:O]\n",
    "    NN_pred_output_P = NN_pred_output[O:]\n",
    "\n",
    "    #Convert to numpy\n",
    "    NN_test_input = NN_test_input.numpy()\n",
    "\n",
    "    #Storing residuals \n",
    "    GP_res_T[NN_test_idx, :] = GP_test_output_T.numpy() - NN_test_output_T\n",
    "    GP_res_P[NN_test_idx, :] = GP_test_output_P.numpy() - NN_test_output_P\n",
    "    NN_res_T[NN_test_idx, :] = NN_pred_output_T - NN_test_output_T\n",
    "    NN_res_P[NN_test_idx, :] = NN_pred_output_P - NN_test_output_P\n",
    "\n",
    "    #Plotting\n",
    "    if (NN_test_idx % substep == 0):\n",
    "        fig, axs = plt.subplot_mosaic([['res_pressure', '.'],\n",
    "                                       ['results', 'res_temperature']],\n",
    "                              figsize=(8, 6),\n",
    "                              width_ratios=(3, 1), height_ratios=(1, 3),\n",
    "                              layout='constrained')        \n",
    "        axs['results'].plot(NN_test_output_T, NN_test_output_P, '.', linestyle='-', color='blue', linewidth=2, label='Truth')\n",
    "        axs['results'].plot(NN_pred_output_T, NN_pred_output_P, color='green', linewidth=2, label='NN prediction')\n",
    "        axs['results'].plot(GP_test_output_T, GP_test_output_P, color='red', linewidth=2, label='GP prediction')\n",
    "        axs['results'].invert_yaxis()\n",
    "        axs['results'].set_ylabel(r'log$_{10}$ Pressure (bar)')\n",
    "        axs['results'].set_xlabel('Temperature (K)')\n",
    "        axs['results'].legend()\n",
    "        axs['results'].grid()\n",
    "\n",
    "        axs['res_temperature'].plot(NN_res_T[NN_test_idx, :], NN_test_output_P, '.', linestyle='-', color='green', linewidth=2)\n",
    "        axs['res_temperature'].plot(GP_res_T[NN_test_idx, :], NN_test_output_P, '.', linestyle='-', color='red', linewidth=2)\n",
    "        axs['res_temperature'].set_xlabel('Residuals (K)')\n",
    "        axs['res_temperature'].invert_yaxis()\n",
    "        axs['res_temperature'].grid()\n",
    "        axs['res_temperature'].axvline(0, color='black', linestyle='dashed', zorder=2)\n",
    "        axs['res_temperature'].yaxis.tick_right()\n",
    "        axs['res_temperature'].yaxis.set_label_position(\"right\")\n",
    "        axs['res_temperature'].sharey(axs['results'])\n",
    "\n",
    "        axs['res_pressure'].plot(NN_test_output_T, NN_res_P[NN_test_idx, :], '.', linestyle='-', color='green', linewidth=2)\n",
    "        axs['res_pressure'].plot(NN_test_output_T, GP_res_P[NN_test_idx, :], '.', linestyle='-', color='red', linewidth=2)\n",
    "        axs['res_pressure'].set_ylabel('Residuals (bar)')\n",
    "        axs['res_pressure'].invert_yaxis()\n",
    "        axs['res_pressure'].grid()\n",
    "        axs['res_pressure'].axhline(0, color='black', linestyle='dashed', zorder=2)\n",
    "        axs['res_pressure'].xaxis.tick_top()\n",
    "        axs['res_pressure'].xaxis.set_label_position(\"top\")\n",
    "        axs['res_pressure'].sharex(axs['results'])\n",
    "\n",
    "        plt.suptitle(rf'H$_2$ : {NN_test_input[0]} bar, CO$_2$ : {NN_test_input[1]} bar, LoD : {NN_test_input[2]:.0f} days, Obliquity : {NN_test_input[3]} deg')\n",
    "        plt.savefig(plot_save_path+f'/pred_vs_actual_n.{NN_test_idx}.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ffd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- GP Residuals ---')\n",
    "print(f'Temperature Residuals : Median = {np.median(GP_res_T):.2f} K, Std = {np.std(GP_res_T):.2f} K')\n",
    "print(rf'Pressure Residuals : Median = {np.median(GP_res_P):.2f} $log_{10}$ bar, Std = {np.std(GP_res_P):.2f} $log_{10}$ bar')\n",
    "print('\\n','--- NN Residuals ---')\n",
    "print(f'Temperature Residuals : Median = {np.median(NN_res_T):.2f} K, Std = {np.std(NN_res_T):.2f} K')\n",
    "print(rf'Pressure Residuals : Median = {np.median(NN_res_P):.3f} $log_{10}$ bar, Std = {np.std(NN_res_P):.2f} $log_{10}$ bar')\n",
    "\n",
    "#Plot residuals\n",
    "fig, ((ax1, ax3),(ax2,ax4)) = plt.subplots(2, 2, sharex=True, figsize=[12, 8])\n",
    "ax1.plot(GP_res_T.T, alpha=0.1, color='green')\n",
    "ax2.plot(GP_res_P.T, alpha=0.1, color='green')\n",
    "ax3.plot(NN_res_T.T, alpha=0.1, color='blue')\n",
    "ax4.plot(NN_res_P.T, alpha=0.1, color='blue')\n",
    "for ax in [ax1, ax2, ax3, ax4]:ax.axhline(0, color='black', linestyle='dashed')\n",
    "ax2.set_xlabel('Index')\n",
    "ax4.set_xlabel('Index')\n",
    "ax1.set_ylabel('Temperature')\n",
    "ax2.set_ylabel('log$_{10}$ Pressure (bar)')\n",
    "ax3.set_ylabel('Temperature')\n",
    "ax4.set_ylabel('log$_{10}$ Pressure (bar)')\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    ax.grid()\n",
    "plt.subplots_adjust(hspace=0.1, bottom=0.25)\n",
    "\n",
    "# Add statistics text at the bottom\n",
    "stats_text = (\n",
    "    f\"--- GP Residuals ---\\n\"\n",
    "    f\"Temperature Residuals : Median = {np.median(GP_res_T):.2f} K, Std = {np.std(GP_res_T):.2f} K\\n\"\n",
    "    f\"Pressure Residuals : Median = {np.median(GP_res_P):.2f} $log_{{10}}$ bar, Std = {np.std(GP_res_P):.2f} $log_{{10}}$ bar\\n\"\n",
    "    f\"\\n\"\n",
    "    f\"--- NN Residuals ---\\n\"\n",
    "    f\"Temperature Residuals : Median = {np.median(NN_res_T):.2f} K, Std = {np.std(NN_res_T):.2f} K\\n\"\n",
    "    f\"Pressure Residuals : Median = {np.median(NN_res_P):.3f} $log_{{10}}$ bar, Std = {np.std(NN_res_P):.2f} $log_{{10}}$ bar\"\n",
    ")\n",
    "\n",
    "fig.text(0.1, 0.05, stats_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig(plot_save_path+f'/res_GP_NN.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
